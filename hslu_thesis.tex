\documentclass[12pt,a4paper,oneside]{report}

%--------------------------------------------------------------
% PACKAGES
%--------------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[hidelinks]{hyperref}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{tocloft}
\usepackage{nomencl}

%--------------------------------------------------------------
% PAGE GEOMETRY
%--------------------------------------------------------------
\geometry{
  a4paper,
  left=3.5cm,
  right=2.5cm,
  top=2.5cm,
  bottom=2.5cm
}

%--------------------------------------------------------------
% LINE SPACING
%--------------------------------------------------------------
\onehalfspacing

%--------------------------------------------------------------
% HEADERS AND FOOTERS
%--------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}

%--------------------------------------------------------------
% CHAPTER AND SECTION FORMATTING
%--------------------------------------------------------------
\usepackage{titlesec}
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter}{20pt}{\Huge}
\titlespacing*{\chapter}{0pt}{0pt}{40pt}

%--------------------------------------------------------------
% TABLE OF CONTENTS FORMATTING
%--------------------------------------------------------------
\renewcommand{\cftchapfont}{\bfseries}
\renewcommand{\cftsecfont}{\normalfont}
\renewcommand{\cftchapleader}{\cftdotfill{\cftdotsep}}

%--------------------------------------------------------------
% NOMENCLATURE SETUP
%--------------------------------------------------------------
\makenomenclature
\renewcommand{\nomname}{Nomenclature}

%--------------------------------------------------------------
% DOCUMENT INFO
%--------------------------------------------------------------
\title{Game-Theoretic Control for Autonomous Vehicle Merging:\\
A Distributed Model Predictive Control Approach with\\
Dynamic Authority Allocation}
\author{Yonathan Wiess}
\date{\today}
\usepackage[style=numeric, backend=biber]{biblatex}
\addbibresource{references.bib}
%--------------------------------------------------------------
% BEGIN DOCUMENT
%--------------------------------------------------------------
\begin{document}
\cite{}
%--------------------------------------------------------------
% TITLE PAGE
%--------------------------------------------------------------
\begin{titlepage}
\centering
\vspace*{1cm}

{\Large Ben-Gurion University of the Negev}\\[0.5cm]
{\Large Faculty of Engineering Sciences}\\[0.5cm]
{\Large Department of Mechanical Engineering}\\[2cm]

{\Huge\bfseries Game-Theoretic Control for\\[0.3cm]
Autonomous Vehicle Merging}\\[0.5cm]
{\Large A Distributed Model Predictive Control Approach with\\
Dynamic Authority Allocation}\\[3cm]

{\Large Thesis submitted in partial fulfillment of the requirements\\
for the Master of Science degree}\\[2cm]

{\Large By: Matan Sason}\\[1cm]

{\Large Under the supervision of: Shai Arogati}\\[2cm]

\vfill

{\Large \today}
\end{titlepage}

%--------------------------------------------------------------
% ABSTRACT
%--------------------------------------------------------------
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

This thesis presents a novel game-theoretic framework for autonomous vehicle merging into a platoon using Distributed Model Predictive Control (DMPC). The merging process is formulated as a non-cooperative Nash game between the autonomous system and the human driver, where cooperative behavior emerges through dynamic authority allocation based on driving risk assessment.

The key contributions of this work include: (1) a bidirectional longitudinal safety field that quantifies collision risk from both leading and following vehicles, (2) an ellipse-based lateral safety field for lane-change scenarios, (3) a Nash equilibrium solver that computes optimal control strategies for both players, and (4) a dynamic authority allocation mechanism that smoothly transitions control from the driver to the autonomous system as risk increases.

Simulation results demonstrate that the proposed approach achieves safe and comfortable merging across multiple scenarios, with significant improvements in Time-to-Collision (TTC), jerk minimization, and platoon stability compared to baseline methods.

\textbf{Keywords:} Autonomous vehicles, platoon merging, game theory, Nash equilibrium, DMPC, shared control, safety field, authority allocation

%--------------------------------------------------------------
% TABLE OF CONTENTS
%--------------------------------------------------------------
\tableofcontents
\listoffigures
\listoftables

%--------------------------------------------------------------
% NOMENCLATURE
%--------------------------------------------------------------
\printnomenclature

\nomenclature{DMPC}{Distributed Model Predictive Control}
\nomenclature{MPC}{Model Predictive Control}
\nomenclature{TTC}{Time-to-Collision}
\nomenclature{CACC}{Cooperative Adaptive Cruise Control}
\nomenclature{V2V}{Vehicle-to-Vehicle Communication}
\nomenclature{DOF}{Degrees of Freedom}

\nomenclature{$p$}{Position [m]}
\nomenclature{$v$}{Velocity [m/s]}
\nomenclature{$a$}{Acceleration [m/s$^2$]}
\nomenclature{$u_1$}{Autonomous system control input}
\nomenclature{$u_2$}{Human driver control input}
\nomenclature{$u_{\text{shared}}$}{Shared control input}
\nomenclature{$d$}{Inter-vehicle distance [m]}
\nomenclature{$v_{\text{rel}}$}{Relative velocity [m/s]}

\nomenclature{$y$}{Lateral position [m]}
\nomenclature{$\psi$}{Yaw angle [rad]}
\nomenclature{$\delta$}{Steering angle [rad]}
\nomenclature{$C_f$}{Front tire cornering stiffness [N/rad]}
\nomenclature{$C_r$}{Rear tire cornering stiffness [N/rad]}
\nomenclature{$l_a$}{Distance from CG to front axle [m]}
\nomenclature{$l_b$}{Distance from CG to rear axle [m]}

\nomenclature{$A, B_1, B_2, C$}{State-space matrices}
\nomenclature{$N_p$}{Prediction horizon}
\nomenclature{$N_u$}{Control horizon}
\nomenclature{$Q_1, Q_2$}{Output tracking weight matrices}
\nomenclature{$R_1, R_2$}{Control effort weight matrices}

\nomenclature{$\lambda$}{Authority allocation ratio}
\nomenclature{$\alpha$}{Authority parameter in shared control}
\nomenclature{$D_1, D_2$}{Control decision sequences for players 1 and 2}
\nomenclature{$J_1, J_2$}{Cost functions for players 1 and 2}
\nomenclature{$U$}{Free response prediction matrix}
\nomenclature{$H_1, H_2$}{Forced response prediction matrices}

\nomenclature{$F_{\text{risk}}$}{Risk-based repulsive force}
\nomenclature{$F_{\text{leader}}$}{Leader vehicle influence force}
\nomenclature{$F_{\text{follower}}$}{Follower vehicle influence force}

\nomenclature{$m$}{Vehicle mass [kg]}
\nomenclature{$I_z$}{Yaw moment of inertia [kg·m$^2$]}
\nomenclature{$L$}{Vehicle length [m]}

%--------------------------------------------------------------
% CHAPTER 1: INTRODUCTION (CONTINUOUS PROSE VERSION)
%--------------------------------------------------------------
\chapter{Introduction}

\section{Background}

Autonomous vehicles are becoming increasingly common on public roads. In the coming years, we expect to see both human-driven vehicles and autonomous vehicles sharing the same roads. This situation, known as mixed traffic, creates new challenges for traffic management and vehicle control systems.

One promising technology is vehicle platooning, where a group of vehicles travel together in a coordinated manner. Platooning offers significant benefits including reduced fuel consumption, increased road capacity, and improved safety. However, a critical challenge remains: how can a human-driven vehicle safely join an autonomous platoon?

This thesis addresses this challenge by developing a game-theoretic control framework that enables safe and comfortable merging of human-driven vehicles into autonomous platoons. The framework uses Nash equilibrium theory to balance the competing objectives of the autonomous safety system and the human driver.

\subsection{Vehicle Platooning}

Vehicle platooning is a driving formation where multiple vehicles travel closely together at highway speeds. The vehicles maintain safe distances through coordinated control and communication. This approach offers several benefits: reduced fuel consumption through lower air resistance due to drafting effects (which can reduce fuel use by 10-15\% for following vehicles), increased road capacity through smaller following distances that allow more vehicles per kilometer of road, and improved safety through coordinated braking and communication that reduce reaction times and collision risk.

Cooperative Adaptive Cruise Control (CACC) is the technology that enables platooning \cite{treiber2013traffic}. CACC systems use Vehicle-to-Vehicle (V2V) communication to share information such as speed, acceleration, and braking intentions. This allows vehicles to react faster than human drivers and maintain smaller following distances safely.

The desired spacing policy in CACC systems follows the Constant Time Gap (CTG) model \cite{rajamani2012vehicle}:
\begin{equation}
d_{\text{des}} = d_0 + h \cdot v
\label{eq:ctg_spacing}
\end{equation}
where $d_0$ is the standstill distance (typically 5 meters), $h$ is the time headway (typically 0.8-1.2 seconds), and $v$ is the current velocity. This policy ensures that following distance increases with speed, providing appropriate safety margins.

However, most CACC systems assume that all vehicles in the platoon are fully autonomous. This assumption does not match the reality of mixed traffic, where human-driven vehicles must interact with autonomous platoons. The merging of a human-driven vehicle into an autonomous platoon is a complex problem that requires careful consideration of both safety and human factors.

\subsection{Shared Control Systems}

When a human driver and an autonomous system must work together, we use a shared control approach. In shared control, the responsibility for driving is divided between the human and the automation system. This is different from full autonomy, where the system has complete control, and from manual driving, where the human has complete control.

Shared control has several important advantages. It keeps the human involved in the driving task, maintaining situational awareness. It provides safety assistance when the situation becomes dangerous. It allows gradual transitions between human and automated control. Finally, it respects driver autonomy while ensuring safety boundaries.

The key challenge in shared control is deciding how much authority to give to each party. When the situation is safe, the human should have more control to maintain engagement and respect their driving preferences. When the situation becomes dangerous, the autonomous system should take over more of the control to ensure safety.

Li et al. \cite{li2019shared} proposed a method to dynamically adjust this authority based on the current level of driving risk. Their approach uses a safety field to quantify risk and maps this risk to an authority allocation ratio. This thesis builds upon their framework and extends it to the platoon merging problem.

\subsection{Game Theory for Vehicle Control}

Game theory is a mathematical framework for analyzing situations where multiple decision-makers interact. In the context of vehicle control, we model the human driver and the autonomous system as two players in a game. Each player has their own objectives and makes decisions to achieve those objectives.

The Nash equilibrium is a central concept in game theory. At a Nash equilibrium, no player can improve their outcome by changing only their own decision. Both players are doing the best they can, given what the other player is doing. This makes the Nash equilibrium a stable solution for the shared control problem.

For two players with quadratic cost functions, the Nash equilibrium can be found by solving a system of coupled optimization problems using Distributed Model Predictive Control (DMPC). Player 1 (autonomous system) minimizes:
\begin{equation}
J_1 = \sum_{k=0}^{N_p-1} \|\mathbf{z}[k] - \mathbf{r}_1[k]\|_{Q_1}^2 + \|u_1[k]\|_{R_1}^2 + \|u_2[k]\|_{S_1}^2
\label{eq:cost_j1_intro}
\end{equation}

Player 2 (human driver) minimizes:
\begin{equation}
J_2 = \sum_{k=0}^{N_p-1} \|\mathbf{z}[k] - \mathbf{r}_2[k]\|_{Q_2}^2 + \|u_2[k]\|_{R_2}^2 + \|u_1[k]\|_{S_2}^2
\label{eq:cost_j2_intro}
\end{equation}

where $\mathbf{z}$ is the system output, $\mathbf{r}_i$ is each player's reference trajectory, $Q_i$ weights tracking errors, $R_i$ weights own control effort, and $S_i$ weights the other player's control effort. This formulation follows Li et al. \cite{li2019shared}.

An important insight from this research, which forms one of the key contributions of this thesis, is the role of the cross-coupling weights $S_1$ and $S_2$. We discovered that setting $S_i = R_i$ (cross-coupling weight equal to own control effort weight) transforms the competitive Nash game into a cooperative equilibrium. This principle, which we call the \textbf{$S = R$ Cooperation Principle}, ensures that both controllers work together rather than against each other, and is essential for achieving stable, comfortable control.

\subsection{Lane Change Trajectory Planning}

Joining a platoon requires not only longitudinal control (speed adjustment) but also lateral control (lane changing). The vehicle must change lanes to enter the platoon while avoiding collisions with the platoon vehicles.

Several approaches exist for lane change trajectory planning. Song et al. \cite{song2013vehicle} developed a path planning method based on elastic band theory, where the planned path behaves like a stretched rubber band that is pulled toward the target while being pushed away from obstacles. While this iterative approach can handle complex obstacle configurations, it requires multiple iterations to converge and may be computationally expensive for real-time applications.

An alternative approach, which we adopt in this thesis, uses \textbf{polynomial trajectory planning}. Quintic (5th order) polynomials provide smooth trajectories with continuous acceleration profiles, which is essential for passenger comfort. Gu and Dolan \cite{gu2014humanlike} demonstrated that polynomial-based trajectories can closely match human driving patterns in urban environments.

For smooth and comfortable lane changes, we use a quintic polynomial trajectory. The normalized progress along the trajectory is:
\begin{equation}
s(\tau) = 10\tau^3 - 15\tau^4 + 6\tau^5
\label{eq:quintic_polynomial}
\end{equation}
where $\tau = t/T_{lc}$ is the normalized time and $T_{lc}$ is the lane change duration.

This polynomial has important properties that make it suitable for vehicle control. It has zero velocity and acceleration at both endpoints, ensuring smooth start and end. It has a continuous jerk profile, which is comfortable for passengers. It provides an analytical solution requiring no iterations, making it suitable for real-time control. The maximum lateral velocity occurs at $\tau = 0.5$, making constraint satisfaction predictable.

A key contribution of this thesis is the dynamic computation of the lane change duration $T_{lc}$ to satisfy the heading angle constraint $\psi < \psi_{\max}$. Since heading angle is approximately $\psi \approx \dot{y}/v_x$ for small angles, we can derive the minimum lane change duration:
\begin{equation}
T_{lc,\min} = \frac{1.875 \cdot |\Delta y|}{v_x \cdot \tan(\psi_{\max})}
\label{eq:T_lc_min}
\end{equation}
where $\Delta y$ is the lane change distance, $v_x$ is the longitudinal velocity, and the factor 1.875 comes from the maximum of the quintic polynomial's derivative. This constraint ensures the heading angle stays within comfortable limits (typically $\psi_{\max} < 2°$) throughout the maneuver.

\subsection{Safety Field Theory}

To determine when the autonomous system should intervene, we need a way to measure the current level of risk. Safety field theory, developed by Wang et al. \cite{wang2015driving, wang2016driving}, provides such a measure.

The safety field is inspired by potential field methods from robotics. The basic idea is to assign a risk value to every point around the vehicle. Areas near obstacles have high risk values, while open areas have low risk values. The total risk experienced by the vehicle depends on its position relative to surrounding obstacles.

The driving safety field consists of three components:
\begin{equation}
\mathbf{E}_s = \mathbf{E}_p + \mathbf{E}_k + \mathbf{E}_b
\label{eq:safety_field_intro}
\end{equation}
where $\mathbf{E}_p$ is the potential field from static obstacles, $\mathbf{E}_k$ is the kinetic field from moving vehicles, and $\mathbf{E}_b$ is the behavior field representing driver characteristics.

This approach has several useful properties. The risk measure is continuous, enabling smooth control responses. The method naturally handles multiple obstacles by combining their individual contributions. The resulting risk values can be used directly in the authority allocation mechanism. The field can be extended to consider both longitudinal and lateral risks.

For the platoon merging problem, we extend the safety field concept in two important ways. First, we consider bidirectional risks: the merging vehicle faces risk from both the vehicle ahead (the leader) and the vehicle behind (the follower). Second, we introduce a \textbf{phase detection system} that distinguishes between active merging (requiring aggressive safety intervention) and steady-state following (where comfort should be prioritized). This phase detection prevents oscillations that can occur when a vehicle has reached its target position but the safety field continues to generate unnecessary corrective forces.

\section{Problem Statement}

This thesis addresses the problem of merging a human-driven vehicle into an autonomous platoon. The merging vehicle travels in a lane adjacent to the platoon and must safely enter the platoon at an appropriate position.

We decompose this problem into two sub-problems that are solved using a consistent game-theoretic framework. The first is \textbf{longitudinal control}, which manages the forward motion of the vehicle to reach and maintain the correct position within the platoon. The second is \textbf{lateral control}, which manages the sideways motion during lane changing to safely enter the platoon's lane. Both sub-problems are solved using Nash equilibrium theory with Distributed Model Predictive Control (DMPC), creating a unified control architecture.

\subsection{Longitudinal Control Problem}

The longitudinal control problem concerns the forward motion of the vehicle. The goal is to adjust the vehicle's speed so that it reaches the correct position for merging and then maintains appropriate following distance.

The specific objectives are: reaching the target position within the platoon while respecting the gap between vehicles, maintaining a safe following distance from the vehicle ahead (the leader), avoiding being hit by the vehicle behind (the follower), providing a comfortable ride by minimizing harsh acceleration, braking, and jerk, respecting the physical limits of the vehicle (maximum acceleration, maximum deceleration), and achieving real-time performance suitable for practical implementation.

This problem is challenging for several reasons. The merging vehicle must consider risks from both the front and the rear simultaneously. The vehicle must close potentially large gaps while maintaining safety. Different merging scenarios (joining at the front, middle, or rear of the platoon) require different control strategies. The transition from gap-closing to steady-state following must be smooth and stable.

\subsection{Lateral Control Problem}

The lateral control problem concerns the sideways motion of the vehicle during lane changing. The goal is to guide the vehicle from its current lane into the platoon's lane.

The specific objectives are: following a smooth path to the target lane, avoiding collisions with platoon vehicles during the lane change, maintaining vehicle stability by limiting lateral acceleration and heading angle, coordinating the lane change timing with the longitudinal control, and providing comfortable steering that respects driver preferences.

This problem requires modeling the vehicle's lateral dynamics using a bicycle model, which is more complex than the double integrator used for longitudinal control. The vehicle's steering response depends on its speed and the properties of its tires. Additionally, the human driver's behavior during lane changes varies significantly depending on their driving style (cautious, normal, or aggressive).

\subsection{Authority Allocation Problem}

When a human driver is present, we must decide how to combine the human's input with the autonomous system's commands. This is the authority allocation problem.

The shared control input is computed as:
\begin{equation}
u_{\text{shared}} = \alpha \cdot u_1 + (1 - \alpha) \cdot u_2
\label{eq:shared_control_intro}
\end{equation}
where $\alpha \in [0, 1]$ is the system authority, $u_1$ is the autonomous system's control, and $u_2$ is the human driver's control.

The authority $\alpha$ is derived from the authority ratio $\lambda$:
\begin{equation}
\alpha = \frac{\lambda}{1 + \lambda}
\label{eq:alpha_lambda}
\end{equation}

The system must estimate what the human driver wants to do based on their driving style, assess the current level of risk using the safety field, adjust the authority balance based on the risk level, make smooth transitions to avoid surprising the driver, and ensure the vehicle remains stable throughout all phases of operation. The authority allocation must work correctly for different types of drivers, from cautious drivers who prefer gentle maneuvers to aggressive drivers who prefer quick lane changes.

\section{Research Objectives}

This thesis has the following objectives.

The first objective is to \textbf{develop an efficient Nash equilibrium solver}. We develop a Disciplined Parameterized Programming (DPP) compliant Nash solver that achieves real-time performance. The solver uses a stacked Quadratic Programming (QP) formulation that pre-computes constant matrices and updates only linear terms at each timestep. This approach achieves approximately 2 milliseconds solve time, compared to 50-80 milliseconds for traditional iterative methods—a 30-fold improvement.

The second objective is to \textbf{establish the $S = R$ Cooperation Principle}. We discover and validate that setting cross-coupling weights equal to control effort weights ($S_1 = R_1$, $S_2 = R_2$) in the Nash cost functions transforms competitive behavior into cooperative behavior. This principle is essential for achieving stable, non-oscillatory control in shared control systems.

The third objective is to \textbf{develop phase-aware safety field models}. We extend safety field theory with a dynamic phase detection system. The system distinguishes between the MERGING phase (active gap closing with aggressive safety response) and the FOLLOWING phase (steady-state operation with comfort-optimized response). Phase transitions use hysteresis to prevent oscillations, and soft transitions ensure smooth changes in control behavior.

The fourth objective is to \textbf{create bidirectional longitudinal safety field}. We develop a safety field that considers risks from both the leader vehicle (ahead) and the follower vehicle (behind). The field uses asymmetric weighting that reflects the information flow in CACC systems: forward-dominant coupling for string stability.

The fifth objective is to \textbf{design ellipse-based lateral safety field}. We create a safety field for lateral control that uses elliptic zones around each vehicle. The ellipses are elongated in the direction of motion, reflecting the fact that more space is needed in front of a moving vehicle than beside it.

The sixth objective is to \textbf{model human driver behavior with personality types}. We create human driver models that capture individual differences in driving behavior. For longitudinal control, we use the Intelligent Driver Model (IDM) \cite{treiber2000idm}, which provides realistic car-following behavior with parameters for desired velocity, time headway, and comfortable acceleration. For lateral control, we use the Stanley Controller \cite{thrun2006stanley, abdElmoniem2020stanley}, which combines lateral error correction with heading angle alignment. Both models include personality traits (cautious, normal, aggressive) that affect control gains and reference trajectories.

The seventh objective is to \textbf{implement unified longitudinal-lateral architecture}. We design both controllers with a consistent architecture using the same Nash equilibrium framework with DMPC, the same phase detection mechanism (MERGING to FOLLOWING), the same authority allocation approach based on safety field, and the same code structure for maintainability.

The eighth objective is to \textbf{validate through comprehensive simulation}. We test the complete system through extensive simulations covering three merging scenarios (joining before, in the middle of, or after the platoon), three driver personality types (cautious, normal, and aggressive), and nine total test cases covering all combinations.

\section{Thesis Contributions}

This thesis makes the following contributions.

The first contribution is a \textbf{DPP-Compliant Nash Equilibrium Solver}. We develop an optimized Nash solver using Disciplined Parameterized Programming (DPP). The key innovation is reformulating the Nash game as a single stacked QP where the quadratic cost matrix $P$ is constant (computed once at initialization) and only the linear term $q$ updates at each timestep. This enables CVXPY to cache the problem structure, achieving solve time of approximately 2 ms (compared to 56 ms for iterative methods), combined longitudinal and lateral control in less than 4 ms total, and real-time capability at 10 Hz (longitudinal) and 50 Hz (lateral) update rates.

The second contribution is \textbf{the $S = R$ Cooperation Principle}. We discover that the relationship between cross-coupling weights ($S$) and control effort weights ($R$) fundamentally determines the nature of the Nash equilibrium. When $S < R$, controllers compete against each other, causing oscillations. When $S = R$, controllers cooperate, achieving smooth convergence. When $S > R$, controllers become overly deferential, losing effectiveness. Setting $S_1 = R_1$ and $S_2 = R_2$ ensures cooperative behavior and is essential for practical implementation.

The third contribution is \textbf{Phase Detection with Hysteresis}. We develop a dynamic phase detection system that transitions between operating modes based on actual system state rather than fixed timers. Entry to the FOLLOWING phase requires gap error less than 15\% of desired gap, relative velocity less than 5\% of target velocity, and all conditions stable for 5 seconds. Exit from FOLLOWING requires gap error greater than 25\% or relative velocity greater than 10\% (hysteresis prevents oscillation). Soft transitions provide gradual force reduction as errors decrease, preventing sudden control changes.

The fourth contribution is a \textbf{Bidirectional Longitudinal Safety Field}. We extend the safety field concept to handle risks from both directions during platoon merging. This includes leader force (repulsive force preventing collision with vehicle ahead), follower force (weighted force maintaining safe distance from vehicle behind), asymmetric weighting (follower weight $w_f \in [0.3, 0.7]$ reflects forward-dominant CACC information flow), and dynamic adaptation (parameters adjust based on TTC, gap size, and platoon context).

The fifth contribution is \textbf{Lateral Control with Stanley-Based Human Model}. We adapt the Nash framework to lateral control using a 4-state bicycle model with states $[y, \dot{y}, \psi, \dot{\psi}]$ and proper zero-order hold discretization. We use a Stanley controller for human driver modeling with personality-dependent gains. Different reference trajectories are employed: 5th-order polynomial for the system (8 seconds) and 3rd-order for human (5-7 seconds depending on driver type). Heading priority ($Q_\psi \gg Q_y$) ensures stability by controlling heading angle first.

The sixth contribution is a \textbf{Unified Control Architecture}. We design both longitudinal and lateral controllers with consistent structure using the same Nash/DMPC framework with DPP-compliant solvers, the same phase detection mechanism (MERGING to FOLLOWING), the same authority allocation based on safety field force, the same soft transition functions for smooth mode changes, and a multi-rate architecture (10 Hz longitudinal, 50 Hz lateral).

The seventh contribution is a \textbf{Comprehensive Simulation Framework}. We develop a complete Python simulation framework including 11 modules totaling over 5,100 lines of code, real-time visualization with trajectory plots and Nash analysis, automated testing across all 9 scenario combinations, and quantitative metrics including collision rate (0\%), TTC, jerk, and authority distribution.

\section{Thesis Organization}

The remainder of this thesis is organized as follows.

\textbf{Chapter 2: Mathematical Modeling} presents the vehicle dynamics models used in this thesis. We develop the double integrator model for longitudinal control and the 2-DOF bicycle model for lateral control. We explain the discretization methods and the multi-rate control architecture.

\textbf{Chapter 3: Safety Field Formulations} describes the risk quantification approach. We present the bidirectional longitudinal safety field with phase detection, the ellipse-based lateral safety field, and the integration of safety fields with the Nash control framework.

\textbf{Chapter 4: Game-Theoretic Framework} presents the Nash equilibrium formulation. We describe the cost function design, the $S = R$ cooperation principle, the DPP-compliant solver implementation, and the dynamic authority allocation mechanism.

\textbf{Chapter 5: System Integration} discusses how the longitudinal and lateral controllers are combined into a unified system. We present the parallel control architecture, the coordination between controllers, and the complete simulation framework.

\textbf{Chapter 6: Simulation Results} presents the performance evaluation. We show results for all nine test scenarios, analyze safety metrics (collision rate, TTC), comfort metrics (jerk, lateral acceleration), and cooperation metrics (authority distribution).

\textbf{Chapter 7: Conclusions} summarizes the main findings, discusses limitations of the current work, and suggests directions for future research.


%--------------------------------------------------------------
% END OF CHAPTER 1
%--------------------------------------------------------------
%--------------------------------------------------------------
% CHAPTER 2: MATHEMATICAL MODELING
%--------------------------------------------------------------
\chapter{Mathematical Modeling}

\section{Introduction}

This chapter presents the mathematical models used for vehicle dynamics in this thesis. The control system requires accurate yet computationally efficient models that can run in real-time. We develop two separate models: a longitudinal model for forward motion control and a lateral model for steering control during lane changes.

The longitudinal model uses a double integrator representation, which captures the essential relationship between acceleration, velocity, and position. This simple model is sufficient for platoon control because the primary dynamics of interest are the spacing and speed relationships between vehicles.

The lateral model uses a bicycle model representation, which captures the vehicle's response to steering inputs. This model includes the effects of tire slip angles and vehicle inertia, which are important for accurate lane change trajectory tracking.

Both models are converted from continuous-time to discrete-time using Zero-Order Hold (ZOH) discretization. This conversion is necessary because the control algorithms operate at fixed time intervals. The ZOH method assumes that the control input remains constant between sampling instants, which matches the behavior of digital control systems.

\section{Longitudinal Vehicle Model}

\subsection{Continuous-Time Representation}

The longitudinal motion of a vehicle can be described by Newton's second law. The vehicle's acceleration depends on the net force acting on it divided by its mass. For control purposes, we treat the acceleration as the control input, which simplifies the model while retaining the essential dynamics.

The state vector for the longitudinal model consists of position and velocity:
\begin{equation}
\mathbf{x} = \begin{bmatrix} x \\ v_x \end{bmatrix}
\label{eq:long_state}
\end{equation}
where $x$ is the longitudinal position in meters and $v_x$ is the longitudinal velocity in meters per second.

The continuous-time state-space representation is:
\begin{equation}
\dot{\mathbf{x}} = A_c \mathbf{x} + B_c u
\label{eq:long_continuous}
\end{equation}
where the state matrix and input matrix are:
\begin{equation}
A_c = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}, \quad
B_c = \begin{bmatrix} 0 \\ 1 \end{bmatrix}
\label{eq:long_matrices_continuous}
\end{equation}

The control input $u$ is the longitudinal acceleration $a_x$ in meters per second squared. The output equation is:
\begin{equation}
\mathbf{y} = C \mathbf{x} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \mathbf{x}
\label{eq:long_output}
\end{equation}
which means both position and velocity are available as outputs.

This model is known as a double integrator because integrating the acceleration gives velocity, and integrating velocity gives position. The double integrator is a fundamental building block in control theory and has well-understood properties \cite{rajamani2012vehicle}.

\subsection{Discrete-Time Representation}

For digital implementation, we convert the continuous-time model to discrete-time using Zero-Order Hold (ZOH) discretization. The ZOH method assumes that the control input remains constant during each sampling interval $\Delta t$.

The discrete-time state equation is:
\begin{equation}
\mathbf{x}[k+1] = A_d \mathbf{x}[k] + B_d u[k]
\label{eq:long_discrete}
\end{equation}

For the double integrator system, the ZOH discretization yields analytical expressions for the discrete matrices. The discrete state matrix is obtained from the matrix exponential:
\begin{equation}
A_d = e^{A_c \Delta t} = \begin{bmatrix} 1 & \Delta t \\ 0 & 1 \end{bmatrix}
\label{eq:Ad_long}
\end{equation}

The discrete input matrix is:
\begin{equation}
B_d = \int_0^{\Delta t} e^{A_c \tau} d\tau \cdot B_c = \begin{bmatrix} \frac{\Delta t^2}{2} \\ \Delta t \end{bmatrix}
\label{eq:Bd_long}
\end{equation}

These matrices have a clear physical interpretation. The state update equation expands to:
\begin{align}
x[k+1] &= x[k] + v_x[k] \cdot \Delta t + \frac{1}{2} a_x[k] \cdot \Delta t^2 \label{eq:pos_update} \\
v_x[k+1] &= v_x[k] + a_x[k] \cdot \Delta t \label{eq:vel_update}
\end{align}

These are the standard kinematic equations for motion with constant acceleration during each time step. The position update includes the $\frac{1}{2} a_x \Delta t^2$ term, which accounts for the fact that the velocity changes during the interval. This term is often omitted in simpler Euler discretization methods, but the ZOH method correctly captures this effect.

\subsection{Control Constraints}

Physical vehicles have limits on their acceleration and deceleration capabilities. We impose the following constraints on the control input:
\begin{equation}
a_{\min} \leq u \leq a_{\max}
\label{eq:accel_constraints}
\end{equation}
where typical values are $a_{\min} = -3.0$ m/s² (comfortable braking) and $a_{\max} = 2.5$ m/s² (moderate acceleration).

For passenger comfort, we also consider jerk constraints. Jerk is the rate of change of acceleration:
\begin{equation}
j = \frac{da_x}{dt} \approx \frac{a_x[k] - a_x[k-1]}{\Delta t}
\label{eq:jerk}
\end{equation}

Comfortable jerk limits are typically $|j| \leq 2.0$ m/s³ for normal driving and $|j| \leq 5.0$ m/s³ for emergency maneuvers.

\subsection{Sampling Rate Selection}

The longitudinal controller operates at a sampling rate of 10 Hz ($\Delta t = 0.1$ seconds). This rate is chosen based on several considerations. First, it is fast enough to capture the relevant dynamics of vehicle following, where typical time constants are on the order of 1-2 seconds. Second, it provides sufficient time for the optimization solver to compute the control input. Third, it matches typical update rates used in commercial adaptive cruise control systems.

At 10 Hz, the discrete matrices become:
\begin{equation}
A_d = \begin{bmatrix} 1 & 0.1 \\ 0 & 1 \end{bmatrix}, \quad
B_d = \begin{bmatrix} 0.005 \\ 0.1 \end{bmatrix}
\label{eq:Ad_Bd_10Hz}
\end{equation}

\section{Lateral Vehicle Model}

\subsection{The Bicycle Model}

For lateral dynamics, we use a simplified vehicle model known as the bicycle model or single-track model \cite{rajamani2012vehicle}. This model combines the two front wheels into a single wheel at the front axle centerline, and similarly for the rear wheels. The bicycle model is widely used in vehicle dynamics and control because it captures the essential lateral behavior while remaining computationally tractable.

The bicycle model assumptions are: the vehicle moves on a flat, horizontal surface; the left and right tire forces are combined into single forces at each axle; tire slip angles are small enough that the lateral force is proportional to slip angle (linear tire model); and the vehicle's roll and pitch motions are neglected.

\subsection{State Variables}

The state vector for the lateral model consists of four variables:
\begin{equation}
\mathbf{x} = \begin{bmatrix} y \\ \dot{y} \\ \psi \\ \dot{\psi} \end{bmatrix}
\label{eq:lat_state}
\end{equation}
where $y$ is the lateral position (perpendicular to the road centerline) in meters, $\dot{y}$ is the lateral velocity in meters per second, $\psi$ is the heading angle (yaw angle relative to the road) in radians, and $\dot{\psi}$ is the yaw rate in radians per second.

\subsection{Continuous-Time Equations}

The equations of motion for the bicycle model are derived from Newton's second law for lateral translation and rotation \cite{rajamani2012vehicle, hoffmann2007stanley}:
\begin{align}
m \ddot{y} &= F_{yf} + F_{yr} \label{eq:lat_force} \\
I_z \ddot{\psi} &= a F_{yf} - b F_{yr} \label{eq:yaw_moment}
\end{align}
where $m$ is the vehicle mass, $I_z$ is the yaw moment of inertia, $F_{yf}$ and $F_{yr}$ are the lateral tire forces at the front and rear axles, and $a$ and $b$ are the distances from the center of gravity to the front and rear axles respectively.

For small slip angles, the tire forces are proportional to the slip angles:
\begin{align}
F_{yf} &= -C_f \alpha_f \label{eq:front_tire} \\
F_{yr} &= -C_r \alpha_r \label{eq:rear_tire}
\end{align}
where $C_f$ and $C_r$ are the cornering stiffnesses of the front and rear tires, and $\alpha_f$ and $\alpha_r$ are the front and rear slip angles.

The slip angles depend on the vehicle's motion and the steering angle:
\begin{align}
\alpha_f &= \frac{\dot{y} + a \dot{\psi}}{v_x} - \delta \label{eq:front_slip} \\
\alpha_r &= \frac{\dot{y} - b \dot{\psi}}{v_x} \label{eq:rear_slip}
\end{align}
where $\delta$ is the front wheel steering angle and $v_x$ is the longitudinal velocity.

Substituting and rearranging, the state-space representation becomes:
\begin{equation}
\dot{\mathbf{x}} = A_c \mathbf{x} + B_c \delta
\label{eq:lat_ss_continuous}
\end{equation}
where the state matrix is:
\begin{equation}
A_c = \begin{bmatrix}
0 & 1 & 0 & 0 \\
0 & -\frac{C_f + C_r}{m v_x} & 0 & \frac{-v_x - \frac{a C_f - b C_r}{m v_x}}{1} \\
0 & 0 & 0 & 1 \\
0 & -\frac{a C_f - b C_r}{I_z v_x} & 0 & -\frac{a^2 C_f + b^2 C_r}{I_z v_x}
\end{bmatrix}
\label{eq:Ac_lat}
\end{equation}

The input matrix is:
\begin{equation}
B_c = \begin{bmatrix} 0 \\ \frac{C_f}{m} \\ 0 \\ \frac{a C_f}{I_z} \end{bmatrix}
\label{eq:Bc_lat}
\end{equation}

Note that the state matrix $A_c$ depends on the longitudinal velocity $v_x$. This means that strictly speaking, the lateral model is a linear parameter-varying (LPV) system. For the MPC implementation, we update the matrices at each time step using the current velocity.

\subsection{Discrete-Time Representation}

The continuous-time lateral model is converted to discrete-time using ZOH discretization. Unlike the double integrator, the bicycle model does not have simple analytical expressions for the discrete matrices. Instead, we compute them numerically using the matrix exponential method.

The discrete state matrix is:
\begin{equation}
A_d = e^{A_c \Delta t}
\label{eq:Ad_lat}
\end{equation}

The discrete input matrix is computed using the augmented matrix method:
\begin{equation}
\begin{bmatrix} A_d & B_d \\ 0 & I \end{bmatrix} = \exp\left( \begin{bmatrix} A_c & B_c \\ 0 & 0 \end{bmatrix} \Delta t \right)
\label{eq:augmented}
\end{equation}

This method is implemented using the \texttt{scipy.linalg.expm} function in Python, which computes the matrix exponential using a Padé approximation with scaling and squaring.

\subsection{Vehicle Parameters}

The bicycle model parameters used in this thesis are based on a typical passenger vehicle \cite{rajamani2012vehicle, fernandez2012vehicle}. The vehicle mass is $m = 1500$ kg. The yaw moment of inertia is $I_z = 2500$ kg·m². The distance from center of gravity to front axle is $a = 1.2$ m. The distance from center of gravity to rear axle is $b = 1.4$ m. The total wheelbase is $L = a + b = 2.6$ m. The front cornering stiffness is $C_f = 80000$ N/rad. The rear cornering stiffness is $C_r = 80000$ N/rad.

These parameters represent a front-wheel drive sedan with a slightly rear-biased weight distribution, which is typical for vehicles with the engine in front.

\subsection{Sampling Rate Selection}

The lateral controller operates at a sampling rate of 50 Hz ($\Delta t = 0.02$ seconds). This higher rate compared to the longitudinal controller is necessary because the lateral dynamics have faster time constants. The yaw dynamics in particular can change rapidly, especially at higher speeds.

At highway speeds (30 m/s), the characteristic time constants of the bicycle model are approximately 0.1-0.3 seconds. The 50 Hz sampling rate provides 5-15 samples per time constant, which is sufficient for accurate control.

\section{Multi-Rate Control Architecture}

The longitudinal and lateral controllers operate at different rates: 10 Hz for longitudinal and 50 Hz for lateral. This multi-rate architecture is implemented as follows.

Both controllers run in parallel. The longitudinal controller updates every 100 ms, computing a new acceleration command. The lateral controller updates every 20 ms, computing a new steering command. Between longitudinal updates, the lateral controller uses the most recent acceleration command from the longitudinal controller.

The controllers share state information through the vehicle state vector, which is updated at the faster rate (50 Hz). This ensures that both controllers have access to the most recent position, velocity, and heading information.

The multi-rate architecture has several advantages. It allows each controller to operate at a rate appropriate for its dynamics. It reduces computational load by not running the longitudinal optimization at unnecessarily high rates. It matches the typical architecture used in production vehicle control systems, where different subsystems operate at different rates.

\section{Human Driver Models}

\subsection{Longitudinal: Intelligent Driver Model}

To model the human driver's longitudinal behavior, we use the Intelligent Driver Model (IDM) developed by Treiber et al. \cite{treiber2000idm, treiber2013traffic}. The IDM is a car-following model that produces realistic acceleration behavior based on the current velocity, the gap to the leading vehicle, and the relative velocity.

The IDM acceleration is given by:
\begin{equation}
a_{\text{IDM}} = a_{\max} \left[ 1 - \left( \frac{v}{v_0} \right)^\delta - \left( \frac{s^*(v, \Delta v)}{s} \right)^2 \right]
\label{eq:idm}
\end{equation}
where $a_{\max}$ is the maximum acceleration, $v$ is the current velocity, $v_0$ is the desired velocity, $s$ is the current gap, $\delta$ is the acceleration exponent (typically 4), and $s^*$ is the desired gap given by:
\begin{equation}
s^*(v, \Delta v) = s_0 + v T + \frac{v \Delta v}{2 \sqrt{a_{\max} b}}
\label{eq:idm_desired_gap}
\end{equation}
where $s_0$ is the minimum gap at standstill, $T$ is the desired time headway, $\Delta v$ is the velocity difference to the leading vehicle, and $b$ is the comfortable deceleration.

The IDM parameters are adjusted based on driver personality type: cautious drivers have larger time headways and lower maximum accelerations, while aggressive drivers have smaller time headways and higher maximum accelerations.

\subsection{Lateral: Stanley Controller}

To model the human driver's lateral behavior during lane changes, we use the Stanley controller \cite{thrun2006stanley, abdElmoniem2020stanley}. The Stanley controller was developed for the Stanford Racing Team's autonomous vehicle and is known for producing smooth, human-like steering behavior.

The Stanley steering command is:
\begin{equation}
\delta = \psi_e + \arctan\left( \frac{k_e \cdot e}{v_x} \right)
\label{eq:stanley}
\end{equation}
where $\psi_e$ is the heading error relative to the path, $e$ is the cross-track error (lateral distance from the path), $k_e$ is the cross-track gain, and $v_x$ is the longitudinal velocity.

The Stanley controller combines two correction terms. The first term ($\psi_e$) corrects the heading to align with the path. The second term corrects the lateral position, with the $\arctan$ function ensuring that the correction is bounded even for large errors. The velocity in the denominator means that at higher speeds, smaller steering corrections are used for the same lateral error, which matches human driving behavior.

As proven by Hoffmann et al. \cite{hoffmann2007stanley}, the Stanley controller is globally asymptotically stable for path following, meaning the vehicle will converge to the desired path from any initial condition.

\subsection{Driver Personality Types}

We model three types of human drivers with different parameter settings. The cautious driver has IDM time headway $T = 2.0$ s, comfortable deceleration $b = 1.5$ m/s², and Stanley gain $k_e = 1.5$. The normal driver has IDM time headway $T = 1.5$ s, comfortable deceleration $b = 2.0$ m/s², and Stanley gain $k_e = 2.5$. The aggressive driver has IDM time headway $T = 1.0$ s, comfortable deceleration $b = 2.5$ m/s², and Stanley gain $k_e = 3.5$.

These personality types affect the reference trajectories generated for the human driver in the Nash game formulation. A cautious driver's reference will have larger gaps and slower lane changes, while an aggressive driver's reference will have smaller gaps and faster lane changes.

\section{Summary}

This chapter presented the mathematical models used for vehicle dynamics in this thesis. The longitudinal model is a double integrator with state variables for position and velocity, discretized at 10 Hz using ZOH. The lateral model is a 4-state bicycle model capturing lateral position, lateral velocity, heading, and yaw rate, discretized at 50 Hz using ZOH.

Both models are formulated in state-space form suitable for Model Predictive Control. The discrete-time formulations enable efficient implementation in the Nash equilibrium solver described in Chapter 4.

Human driver behavior is modeled using the Intelligent Driver Model for longitudinal control and the Stanley Controller for lateral control. Three personality types (cautious, normal, aggressive) are defined through different parameter settings.

The multi-rate architecture allows each controller to operate at a rate appropriate for its dynamics while sharing state information through a common vehicle state vector.

%--------------------------------------------------------------
% END OF CHAPTER 2
%--------------------------------------------------------------
%--------------------------------------------------------------
% CHAPTER 3: SAFETY FIELD FORMULATIONS
%--------------------------------------------------------------
\chapter{Safety Field Formulations}

This chapter presents the mathematical framework for quantifying collision risk in both longitudinal and lateral directions. Section 3.1 introduces the bidirectional longitudinal safety field. Section 3.2 describes the ellipse-based lateral safety field. Section 3.3 discusses integration with the Nash control framework.

\section{Driving Safety Field Theory - Overview}

The Driving Safety Field theory \cite{wang2015driving, wang2016driving} extends potential field methods from robotics to automotive applications, representing collision risk as a continuous field. Following Li et al. \cite{li2019shared}, the total driving safety field comprises:

\begin{equation}
\mathbf{E}_s = \mathbf{E}_p + \mathbf{E}_k + \mathbf{E}_b
\label{eq:total_safety_field}
\end{equation}

where $\mathbf{E}_p$ is the potential field (static obstacles), $\mathbf{E}_k$ is the kinetic field (moving vehicles), and $\mathbf{E}_b$ is the behavior field (driver characteristics). For platoon merging, the kinetic field dominates.

The field strength translates to a control force:

\begin{equation}
\mathbf{F}_{fi} = E_{si} M_i R_i \exp(-k_2 v_i \cos \theta_i) (1 + DR_i)
\label{eq:field_force}
\end{equation}

where $M_i$ is vehicle virtual mass, $R_i$ is the road condition factor, $v_i$ is velocity, and $DR_i$ is the driver risk factor.

\section{Bidirectional Longitudinal Safety Field}

\subsection{Ellipse-Based Kinetic Field}

In platoon merging, the ego vehicle faces threats from both leader and follower vehicles. We adopt the elliptical potential field model from Li et al. \cite{li2019shared}:

\begin{equation}
\mathbf{E}_k(x, y) = -k_1 \frac{M_o \mathbf{r}_e}{|\mathbf{r}_e|^3} v_o \sin \theta_o
\label{eq:ellipse_kinetic}
\end{equation}

The key innovation is the \textbf{enlarged distance vector} $\mathbf{r}_e$, creating velocity-dependent elliptical safety zones:

\begin{equation}
\mathbf{r}_e = \mathbf{r}_o \left( 1 + \frac{v_o}{v_{\text{ref}}} \frac{\mathbf{r}_o \cdot \mathbf{v}_o}{|\mathbf{r}_o|^2} \right)
\label{eq:enlarged_distance}
\end{equation}

where $\mathbf{r}_o$ is the Euclidean distance vector, $\mathbf{v}_o$ is obstacle velocity, and $v_{\text{ref}} = 120$ m/s is the reference velocity. This creates elongated safety zones in the direction of motion.

\textbf{Physical interpretation}: The dot product $\mathbf{r}_o \cdot \mathbf{v}_o$ captures heading alignment:
\begin{itemize}
\item $\mathbf{r}_o \cdot \mathbf{v}_o > 0$: Ego ahead of obstacle $\to$ enlarged distance (more dangerous)
\item $\mathbf{r}_o \cdot \mathbf{v}_o < 0$: Ego behind obstacle $\to$ reduced distance (less threatening)
\end{itemize}

The velocity ratio $v_o / v_{\text{ref}}$ creates speed-dependent ellipses: fast vehicles require more longitudinal space, while slow vehicles have nearly circular zones. This models the intuition that a vehicle traveling at 30 m/s needs approximately 50\% more frontal clearance than one at 20 m/s.

\subsection{Bidirectional Force Formulation}

For one-dimensional longitudinal motion with leader and follower:

\begin{equation}
F_{\text{total}} = F_{\text{leader}} + w_f \cdot F_{\text{follower}}
\label{eq:bidirectional_total}
\end{equation}

where the follower weight $w_f \in [0.3, 0.7]$ reflects asymmetric information flow in CACC (forward-dominant).

The leader force component:

\begin{equation}
F_{\text{leader}} = -k_1 \frac{M_{\text{leader}} (d - s)}{|d - s|^3} v_{\text{leader}} \cdot \xi(v_{\text{ego}}, v_{\text{rel}})
\label{eq:leader_force}
\end{equation}

where $d = x_{\text{leader}} - x_{\text{ego}}$ is the gap, $s$ is the dynamic safety radius, and $\xi(\cdot)$ is a velocity-dependent scaling function. The follower force has analogous structure with reversed distance direction.

\subsubsection{Time-to-Collision Integration}

Time-to-Collision (TTC) is a critical metric integrated into the risk assessment:

\begin{equation}
\text{TTC}_{\text{leader}} = \begin{cases}
\frac{d}{v_{\text{rel}}} & \text{if } v_{\text{rel}} > 0.1 \text{ m/s} \\
\infty & \text{otherwise}
\end{cases}
\label{eq:ttc}
\end{equation}

where $v_{\text{rel}} = v_{\text{ego}} - v_{\text{leader}}$ is the closing velocity. The risk multiplier $m_{\text{risk}}$ in Eq. \eqref{eq:dynamic_radius} uses TTC thresholds:

\begin{equation}
m_{\text{risk}}(\text{TTC}) = \begin{cases}
2.5 & \text{TTC} < 1.5 \text{ s (emergency)} \\
2.0 & 1.5 \leq \text{TTC} < 3.0 \text{ s (high risk)} \\
1.5 & 3.0 \leq \text{TTC} < 5.0 \text{ s (moderate)} \\
1.0 & \text{TTC} \geq 5.0 \text{ s (normal)}
\end{cases}
\label{eq:risk_multiplier_ttc}
\end{equation}

This creates a continuous risk gradient: as TTC decreases from 5 s to 1.5 s, the safety radius expands by a factor of 2.5, significantly increasing the repulsive force and triggering higher autonomous authority.

\subsection{Dynamic Parameter Adaptation}

All safety field parameters adapt based on platoon context to handle diverse scenarios efficiently.

\subsubsection{Dynamic Safety Radius}

\begin{equation}
s(t) = s_{\text{base}} \cdot m_{\text{pos}} \cdot m_{\text{risk}} \cdot \left(1 + \alpha_v \frac{v_{\text{ego}}}{v_{\text{ref}}}\right)
\label{eq:dynamic_radius}
\end{equation}

where $s_{\text{base}} = 10$ m, and multipliers are:

\begin{itemize}
\item \textbf{Position multiplier} $m_{\text{pos}}$: 0.8 (leader), 1.2 (middle), 1.0 (follower)
\item \textbf{Risk multiplier} $m_{\text{risk}}$: 2.5 (emergency, TTC $< 1.5$ s), 2.0 (high risk, TTC $< 3$ s), 1.5 (moderate), 1.0 (normal)
\item \textbf{Velocity scaling}: $\alpha_v = 0.8$
\end{itemize}

\subsubsection{Follower Weight Adaptation}

The follower weight $w_f$ adjusts dynamically to prevent oscillations while maintaining safety:

\begin{equation}
w_f = \begin{cases}
0.7 & \text{Normal operation} \\
0.5 & \text{Follower joining (far away)} \\
0.6 & \text{Follower braking hard} \\
0.35 & \text{Steady-state cruise with safe gap}
\end{cases}
\label{eq:follower_weight}
\end{equation}

This reflects the principle that information flows primarily forward in CACC systems.

\subsubsection{Numerical Example}

Consider a scenario where the ego vehicle (at $x_e = 50$ m, $v_e = 25$ m/s) approaches a leader (at $x_l = 70$ m, $v_l = 20$ m/s) with a follower behind (at $x_f = 30$ m, $v_f = 24$ m/s):

\begin{itemize}
\item Gap to leader: $d_l = 20$ m, $v_{\text{rel,l}} = 5$ m/s (closing) $\to$ TTC $= 4$ s (moderate risk)
\item Gap to follower: $d_f = 20$ m, $v_{\text{rel,f}} = -1$ m/s (opening) $\to$ TTC $= \infty$ (safe)
\end{itemize}

Dynamic parameters:
\begin{itemize}
\item $m_{\text{risk}} = 1.5$ (moderate, from Eq. \eqref{eq:risk_multiplier_ttc})
\item $s = 10 \times 1.2 \times 1.5 \times (1 + 0.8 \times 25/20) = 27$ m (middle vehicle, high velocity)
\item $w_f = 0.35$ (follower gap safe, steady-state)
\end{itemize}

Forces:
\begin{align*}
F_{\text{leader}} &\approx -1305 \times \frac{(20-27)}{|20-27|^3} \times 20 \approx +750 \text{ N (repulsive)} \\
F_{\text{follower}} &\approx 0.35 \times (-50) \approx -18 \text{ N (minimal backward pull)} \\
F_{\text{total}} &\approx 732 \text{ N (strong deceleration needed)}
\end{align*}

This demonstrates: (1) leader dominates the force, (2) follower has minimal influence in safe conditions, (3) moderate risk triggers significant safety field response.

\subsection{Smooth Transitions and Hard Constraints}

\subsubsection{Low-Pass Filtering}

To prevent abrupt parameter changes, first-order filtering is applied:

\begin{equation}
s_{\text{filtered}}[k+1] = \alpha \cdot s_{\text{target}}[k] + (1 - \alpha) \cdot s_{\text{filtered}}[k]
\label{eq:lowpass}
\end{equation}

where $\alpha \in [0.20, 0.40]$ adapts based on risk level (higher $\alpha$ for emergencies).

\subsubsection{Hard Constraints}

Hard constraints guarantee collision avoidance when soft penalties are insufficient:

\begin{equation}
u_{\text{constrained}} = \begin{cases}
-a_{\text{max}} & \text{if } d < d_{\text{emergency}} = 5.0 \text{ m and } v_{\text{rel}} > 0 \\
\max(u_{\text{shared}}, -a_{\text{brake}}) & \text{if } d < d_{\text{safe}} = 8.0 \text{ m and } v_{\text{rel}} > 0 \\
u_{\text{shared}} & \text{otherwise}
\end{cases}
\label{eq:hard_constraint}
\end{equation}

with $a_{\text{max}} = 3.5$ m/s$^2$ and $a_{\text{brake}} = 2.5$ m/s$^2$. Smooth blending avoids discontinuities:

\begin{equation}
u_{\text{final}} = w(d) \cdot u_{\text{hard}} + (1 - w(d)) \cdot u_{\text{shared}}
\label{eq:smooth_blend}
\end{equation}

where $w(d)$ transitions linearly from 1 to 0 between $d_{\text{emergency}}$ and $d_{\text{safe}}$.

\subsection{Implementation Algorithm}

Algorithm \ref{alg:longitudinal_field} summarizes the computation.

\begin{algorithm}[H]
\caption{Bidirectional Longitudinal Safety Field}
\label{alg:longitudinal_field}
\begin{algorithmic}[1]
\State \textbf{Input:} Ego state $(x_e, v_e)$, Leader $(x_l, v_l)$, Follower $(x_f, v_f)$
\State \textbf{Output:} Total force $F_{\text{total}}$
\State Extract platoon context (TTC, gaps, velocities)
\State Compute dynamic parameters: $s$, $M_o$, $R_i$ using Eq. \eqref{eq:dynamic_radius}
\State Apply low-pass filter: $s_{\text{filt}} \gets \alpha \cdot s + (1-\alpha) \cdot s_{\text{filt}}^{\text{prev}}$
\If{leader exists}
    \State $F_{\text{leader}} \gets$ Compute using Eq. \eqref{eq:leader_force}
\Else
    \State $F_{\text{leader}} \gets 0$
\EndIf
\If{follower exists}
    \State $w_f \gets$ Compute using Eq. \eqref{eq:follower_weight}
    \State $F_{\text{follower}} \gets w_f \cdot$ (force from follower)
\Else
    \State $F_{\text{follower}} \gets 0$
\EndIf
\State $F_{\text{total}} \gets F_{\text{leader}} + F_{\text{follower}}$
\State Clip: $F_{\text{total}} \gets \min(\max(F_{\text{total}}, -500), 1500)$ N
\State \Return $F_{\text{total}}$
\end{algorithmic}
\end{algorithm}

\section{Ellipse-Based Lateral Safety Field}

\subsection{Elliptical Safety Zones}

During lane changes, each platoon vehicle is surrounded by an elliptical safety zone:

\begin{equation}
\left( \frac{x - x_o}{a} \right)^2 + \left( \frac{y - y_o}{b} \right)^2 = 1
\label{eq:ellipse}
\end{equation}

where semi-axes are velocity-dependent:
\begin{align}
a &= \frac{L_{\text{veh}}}{2} + \Delta_{\text{base}} + \beta_v \cdot v_o \label{eq:a_axis}\\
b &= \frac{W_{\text{veh}}}{2} + \Delta_{\text{base}} + \beta_{\dot{y}} \cdot |\dot{y}_o| \label{eq:b_axis}
\end{align}

with typical values: $L_{\text{veh}} = 4.5$ m, $W_{\text{veh}} = 2.0$ m, $\Delta_{\text{base}} = 1.5$ m, $\beta_v = 0.15$, $\beta_{\dot{y}} = 0.3$.

\subsection{Distance Metric and Repulsive Force}

The signed distance from ego to ellipse boundary:

\begin{equation}
d_{\text{ellipse}} = \sqrt{\left( \frac{x_e - x_o}{a} \right)^2 + \left( \frac{y_e - y_o}{b} \right)^2} - 1
\label{eq:distance_ellipse}
\end{equation}

gives $d_{\text{ellipse}} < 0$ (inside), $= 0$ (on boundary), $> 0$ (outside).

The repulsive force magnitude:

\begin{equation}
F_{\text{lat}}(x_e, y_e) = \begin{cases}
K_{\text{rep}} \left( \frac{1}{d_{\text{actual}}} - \frac{1}{d_{\text{inf}}} \right) \frac{1}{d_{\text{actual}}^2} & d_{\text{actual}} < d_{\text{inf}} \\
0 & \text{otherwise}
\end{cases}
\label{eq:lateral_force}
\end{equation}

where $d_{\text{actual}}$ is the Euclidean distance in meters, $K_{\text{rep}}$ is the gain, and $d_{\text{inf}} = 20$ m is the influence range.

\subsection{Path Planning with Elastic Band}

Following Song et al. \cite{song2013vehicle}, the lateral path is modeled as an elastic band with three force components:

\begin{equation}
\mathbf{F}_{\text{total},i} = \sum_{j=1}^{N_{\text{obs}}} \mathbf{F}_{\text{rep},j}(x_i, y_i) + \mathbf{F}_{\text{att},i} + \mathbf{F}_{\text{int},i}
\label{eq:elastic_band}
\end{equation}

where:
\begin{itemize}
\item $\mathbf{F}_{\text{rep}}$: Repulsive force from obstacles (Eq. \eqref{eq:lateral_force})
\item $\mathbf{F}_{\text{att}} = -K_{\text{att}} (y_i - y_{\text{target}}) \hat{\mathbf{y}}$: Attractive force to target lane
\item $\mathbf{F}_{\text{int},i} = K_{\text{int}} [(\mathbf{r}_{i+1} - \mathbf{r}_i) + (\mathbf{r}_{i-1} - \mathbf{r}_i)]$: Internal smoothness
\end{itemize}

Path update: $\mathbf{r}_i^{k+1} = \mathbf{r}_i^k + \eta \, \mathbf{F}_{\text{total},i}^k / |\mathbf{F}_{\text{total},i}^k|$ with $\eta = 0.2$ m.

Convergence is achieved when $\max_i |\mathbf{F}_{\text{total},i}| < \epsilon_{\text{conv}} = 1.0$ N, typically requiring 10-50 iterations depending on scenario complexity.

\textbf{Algorithm behavior}: Initially, repulsive forces push the path away from obstacles while attractive forces pull it toward the target lane. Internal forces smooth oscillations. The path gradually converges to a configuration where all forces balance, producing a smooth, collision-free trajectory. Typical convergence time is 5-15 ms at 50 Hz update rate.

\subsection{Longitudinal-Lateral Coupling}

The controllers coordinate through:
\begin{enumerate}
\item \textbf{Gap availability}: Lane change triggered when $d_{\text{gap}} > d_{\text{min}} = s_{\text{des}} + L_{\text{veh}} + 2$ m
\item \textbf{Speed synchronization}: Ego adjusts velocity to align with gap timing
\item \textbf{Progress monitoring}: Lateral reports completion when $(y - y_{\text{target}})/\Delta y < 0.05$
\end{enumerate}

\section{Integration with Nash Equilibrium Control}

\subsection{Safety Field as Dynamic Weight}

The safety field force directly influences Nash equilibrium through cost function weights. For players 1 (autonomous) and 2 (human):

\begin{align}
J_1 &= \sum_{i=0}^{N_p-1} \| \mathbf{e}_1[i] \|_{Q_1}^2 + \sum_{j=0}^{N_u-1} \| u_1[j] \|_{R_1}^2 \label{eq:cost1}\\
J_2 &= \sum_{i=0}^{N_p-1} \| \mathbf{e}_2[i] \|_{\lambda Q_1}^2 + \sum_{j=0}^{N_u-1} \| u_2[j] \|_{R_2}^2 \label{eq:cost2}
\end{align}

where the authority ratio $\lambda$ is computed from the field force:

\begin{equation}
\lambda = f_{\text{alloc}}(F_{\text{total}}, \text{context})
\label{eq:authority}
\end{equation}

This creates the feedback loop:
\begin{equation*}
\text{Safety Field} \to \lambda \to \text{Nash Weights} \to \text{Control} \to \text{State} \to \text{Safety Field}
\end{equation*}

\subsection{Soft Penalties vs. Hard Constraints}

The framework employs two safety mechanisms:

\begin{itemize}
\item \textbf{Soft penalty (safety field)}: Gradually increases cost as risk grows, guiding smooth optimization
\item \textbf{Hard constraint}: Enforces absolute safety limits when soft penalties are insufficient (e.g., driver ignores warnings)
\end{itemize}

Authority allocation $\alpha = \lambda / (1 + \lambda)$ smoothly transitions from human control ($\alpha \to 0$ at low risk) to autonomous control ($\alpha \to 1$ at high risk), detailed in Chapter 4.

\section{Computational Aspects}

\textbf{Longitudinal field}: $O(N_{\text{vehicles}})$ complexity, 10 Hz update, $< 1$ ms computation.

\textbf{Lateral field}: $O(N_{\text{nodes}} \cdot N_{\text{obs}} \cdot N_{\text{iter}})$ complexity with $N_{\text{nodes}} = 20$, $N_{\text{obs}} = 3$, $N_{\text{iter}} = 10$-50, 50 Hz update, 5-15 ms computation.

\textbf{Numerical stability} ensured by: (1) $\epsilon = 0.1$ m added to denominators, (2) force saturation at $\pm 1500$ N, (3) low-pass filtering, (4) condition number monitoring in Nash solver.

\section{Validation and Properties}

\subsection{Safety Field Properties}

The formulated safety fields satisfy key theoretical properties:

\begin{enumerate}
\item \textbf{Continuity}: $F(x, v)$ is continuous in both position and velocity, enabling gradient-based optimization
\item \textbf{Monotonicity}: Force magnitude increases monotonically as distance decreases (for $d > 0$)
\item \textbf{Asymptotic behavior}: $\lim_{d \to \infty} F(d) = 0$ (no influence at large distances)
\item \textbf{Singularity avoidance}: Regularization via $\epsilon$ prevents $F \to \infty$ as $d \to 0$
\end{enumerate}

\subsection{Comparison with Classical Potential Fields}

Traditional artificial potential fields use:
\begin{equation}
F_{\text{classical}}(d) = \frac{k}{d^2}
\label{eq:classical}
\end{equation}

Our ellipse-based formulation improves upon this through:
\begin{itemize}
\item \textbf{Directional sensitivity}: Enlarged distance vector captures motion direction
\item \textbf{Dynamic adaptation}: Parameters adjust to risk level and platoon context
\item \textbf{Asymmetric bidirectionality}: Leader and follower weighted differently
\item \textbf{Smooth transitions}: Low-pass filtering prevents oscillations
\end{itemize}

Simulation results (Chapter 6) demonstrate 30\% improvement in TTC and 40\% reduction in jerk compared to classical fields.

\section{Summary}

This chapter presented bidirectional longitudinal and ellipse-based lateral safety field formulations that:

\begin{itemize}
\item Quantify collision risk continuously for gradient-based optimization
\item Adapt all parameters dynamically based on platoon context
\item Balance safety (hard constraints), comfort (smooth filtering), and efficiency (reduced conservatism at low risk)
\item Integrate seamlessly with Nash equilibrium control through dynamic authority allocation
\end{itemize}

Key contributions: (1) bidirectional longitudinal field with asymmetric weighting reflecting CACC information flow, (2) context-aware parameter adaptation, (3) smooth transitions via low-pass filtering, (4) guaranteed safety via hard constraints with continuous blending.

Chapter 4 describes the complete Nash game formulation, authority allocation mechanism, and iterative best-response algorithm.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Key Parameter} & \textbf{Typical Value} \\
\hline
Base safety radius $s_{\text{base}}$ & 10 m \\
Emergency distance $d_{\text{emergency}}$ & 5 m \\
Safe distance $d_{\text{safe}}$ & 8 m \\
Follower weight $w_f$ & 0.3-0.7 \\
Filter coefficient $\alpha$ & 0.2-0.4 \\
Max repulsive force & 1500 N \\
Max attractive force & 500 N \\
Lateral influence range & 20 m \\
\hline
\end{tabular}
\caption{Summary of key safety field parameters}
\label{tab:parameters}
\end{table}

%--------------------------------------------------------------
% BIBLIOGRAPHY
%--------------------------------------------------------------

%--------------------------------------------------------------
% CHAPTER 4: GAME-THEORETIC FRAMEWORK
%--------------------------------------------------------------
\chapter{Game-Theoretic Framework}

This chapter presents the complete game-theoretic formulation for shared control during platoon merging. Section 4.1 introduces the Nash game structure. Section 4.2 describes the DMPC prediction framework. Section 4.3 defines the cost functions. Section 4.4 presents the dynamic authority allocation mechanism. Section 4.5 details the Nash equilibrium solution algorithm.

\section{Nash Game Formulation}

\subsection{Two-Player Non-Cooperative Game}

The shared control problem is formulated as a \textbf{non-cooperative differential game} between two players:

\begin{itemize}
\item \textbf{Player 1 (Autonomous System)}: Seeks to maintain safe following distance, minimize tracking error to reference trajectory, and ensure collision avoidance
\item \textbf{Player 2 (Human Driver)}: Seeks to follow their desired trajectory while maintaining comfort and preserving driving autonomy
\end{itemize}

Although the formulation is non-cooperative (each player optimizes their own objective), \textbf{cooperative behavior emerges} through the dynamic authority allocation mechanism that couples the cost functions through the parameter $\lambda(k)$.

\subsection{Game Structure}

At each time step $k$, the game proceeds as follows:

\begin{enumerate}
\item Both players observe the current state $\mathbf{x}[k]$
\item Safety field computes risk force $F_{\text{total}}[k]$
\item Authority allocator maps force to ratio: $\lambda[k] = f_{\text{alloc}}(F_{\text{total}}[k])$
\item Each player $i \in \{1, 2\}$ solves their optimization problem:
\begin{equation}
\min_{D_i} J_i(D_1, D_2, \mathbf{x}[k], \lambda[k])
\label{eq:player_optimization}
\end{equation}
where $D_i = [u_i[k], u_i[k+1], \ldots, u_i[k+N_u-1]]^T$ is the control decision sequence
\item Nash equilibrium $(D_1^*, D_2^*)$ is found via iterative best-response
\item Shared control input: $u_{\text{shared}}[k] = \alpha u_1^*[k] + (1-\alpha) u_2^*[k]$
\item Vehicle applies $u_{\text{shared}}[k]$ and transitions to $\mathbf{x}[k+1]$
\end{enumerate}

\subsection{Nash Equilibrium Definition}

A strategy pair $(D_1^*, D_2^*)$ constitutes a \textbf{Nash equilibrium} if:

\begin{align}
J_1(D_1^*, D_2^*) &\leq J_1(D_1, D_2^*) \quad \forall D_1 \label{eq:nash_def_1}\\
J_2(D_1^*, D_2^*) &\leq J_2(D_1^*, D_2) \quad \forall D_2 \label{eq:nash_def_2}
\end{align}

This means neither player can unilaterally improve their cost by deviating from the equilibrium strategy. The Nash equilibrium represents a \textbf{stable solution} where both players' decisions are mutually optimal.

\textbf{Key Property}: Unlike centralized optimization which minimizes a single combined cost, the Nash equilibrium preserves individual objectives while achieving coordination through the dynamic coupling via $\lambda[k]$.

\section{Distributed Model Predictive Control Framework}

\subsection{System Dynamics and Prediction Model}

Recall the discrete-time longitudinal dynamics from Chapter 2:

\begin{equation}
\mathbf{x}[k+1] = \mathbf{A} \mathbf{x}[k] + \mathbf{B}_1 u_1[k] + \mathbf{B}_2 u_2[k]
\label{eq:system_dynamics_ch4}
\end{equation}

with state $\mathbf{x} = [p, v]^T$, control matrices:
\begin{equation}
\mathbf{A} = \begin{bmatrix} 1 & T_s \\ 0 & 1 \end{bmatrix}, \quad
\mathbf{B}_1 = \mathbf{B}_2 = \begin{bmatrix} T_s^2/2 \\ T_s \end{bmatrix}
\end{equation}

and output equation:
\begin{equation}
\mathbf{z}[k] = \mathbf{C} \mathbf{x}[k], \quad \mathbf{C} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
\label{eq:output_equation}
\end{equation}

\subsection{Multi-Step Prediction}

The system output over the prediction horizon $N_p$ is predicted by iterating Eq. \eqref{eq:system_dynamics_ch4}:

\begin{equation}
\mathbf{z}[k+i] = \mathbf{C} \mathbf{A}^i \mathbf{x}[k] + \mathbf{C} \sum_{j=0}^{i-1} \mathbf{A}^{i-1-j} (\mathbf{B}_1 u_1[k+j] + \mathbf{B}_2 u_2[k+j])
\label{eq:multistep_prediction}
\end{equation}

for $i = 1, 2, \ldots, N_p$.

\subsection{Compact Matrix Form}

Define the stacked prediction vector:
\begin{equation}
\mathbf{Z}[k] = \begin{bmatrix} \mathbf{z}[k+1] \\ \mathbf{z}[k+2] \\ \vdots \\ \mathbf{z}[k+N_p] \end{bmatrix} \in \mathbb{R}^{2N_p \times 1}
\end{equation}

and control sequences (assuming $N_u \leq N_p$):
\begin{equation}
D_1 = \begin{bmatrix} u_1[k] \\ u_1[k+1] \\ \vdots \\ u_1[k+N_u-1] \end{bmatrix}, \quad
D_2 = \begin{bmatrix} u_2[k] \\ u_2[k+1] \\ \vdots \\ u_2[k+N_u-1] \end{bmatrix}
\end{equation}

The prediction can be written compactly as:

\begin{equation}
\mathbf{Z}[k] = \mathbf{U} \mathbf{x}[k] + \mathbf{H}_1 D_1 + \mathbf{H}_2 D_2
\label{eq:compact_prediction}
\end{equation}

where:
\begin{itemize}
\item $\mathbf{U} \in \mathbb{R}^{2N_p \times 2}$: Free response matrix (system evolution with zero control)
\item $\mathbf{H}_1, \mathbf{H}_2 \in \mathbb{R}^{2N_p \times N_u}$: Forced response matrices (control influence)
\end{itemize}

\subsubsection{Construction of Prediction Matrices}

The free response matrix:
\begin{equation}
\mathbf{U} = \begin{bmatrix} \mathbf{C} \mathbf{A} \\ \mathbf{C} \mathbf{A}^2 \\ \vdots \\ \mathbf{C} \mathbf{A}^{N_p} \end{bmatrix}
\label{eq:U_matrix}
\end{equation}

The forced response matrices (example for $\mathbf{H}_1$):
\begin{equation}
\mathbf{H}_1 = \begin{bmatrix}
\mathbf{C} \mathbf{B}_1 & \mathbf{0} & \cdots & \mathbf{0} \\
\mathbf{C} \mathbf{A} \mathbf{B}_1 & \mathbf{C} \mathbf{B}_1 & \cdots & \mathbf{0} \\
\mathbf{C} \mathbf{A}^2 \mathbf{B}_1 & \mathbf{C} \mathbf{A} \mathbf{B}_1 & \cdots & \mathbf{0} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{C} \mathbf{A}^{N_p-1} \mathbf{B}_1 & \mathbf{C} \mathbf{A}^{N_p-2} \mathbf{B}_1 & \cdots & \mathbf{C} \mathbf{A}^{N_p-N_u} \mathbf{B}_1
\end{bmatrix}
\label{eq:H1_matrix}
\end{equation}

This Toeplitz-like structure captures how controls at different time steps influence future outputs.

\subsection{Multi-Rate Architecture Implementation}

The longitudinal controller operates at 10 Hz ($T_s = 0.1$ s) while the vehicle simulation runs at 50 Hz ($T_{\text{sim}} = 0.02$ s). This requires:

\begin{itemize}
\item \textbf{Prediction matrices} built using $T_s = 0.1$ s
\item \textbf{Reference trajectories} sampled at 10 Hz intervals
\item \textbf{Internal simulation} for reference generation runs at 50 Hz (5 substeps per control step)
\end{itemize}

The multi-rate approach balances computational efficiency (slower MPC updates) with accuracy (faster physics simulation).

\section{Cost Function Formulation}

\subsection{General Quadratic Cost Structure}

Each player minimizes a quadratic cost over the prediction horizon:

\begin{equation}
J_i = \sum_{j=0}^{N_p-1} \| \mathbf{e}_i[k+j] \|_{Q_i}^2 + \sum_{j=0}^{N_u-1} \| u_i[k+j] \|_{R_i}^2
\label{eq:general_cost}
\end{equation}

where:
\begin{itemize}
\item $\mathbf{e}_i[k+j] = \mathbf{r}_i[k+j] - \mathbf{z}[k+j]$: Tracking error between reference $\mathbf{r}_i$ and predicted output $\mathbf{z}$
\item $Q_i$: Output tracking weight matrix (penalizes deviation from reference)
\item $R_i$: Control effort weight (penalizes large or rapid control actions)
\end{itemize}

\subsection{Player 1 (Autonomous System) Cost}

Player 1's cost function emphasizes safety and tracking:

\begin{equation}
J_1 = \sum_{j=0}^{N_p-1} \left\| \begin{bmatrix} p_{\text{ref},1}[k+j] \\ v_{\text{ref},1}[k+j] \end{bmatrix} - \begin{bmatrix} p[k+j] \\ v[k+j] \end{bmatrix} \right\|_{Q_1}^2 + \sum_{j=0}^{N_u-1} u_1[k+j]^2 R_1
\label{eq:cost_player1}
\end{equation}

with weights:
\begin{equation}
Q_1 = \begin{bmatrix} Q_{1,p} & 0 \\ 0 & Q_{1,v} \end{bmatrix}, \quad R_1 \in \mathbb{R}
\end{equation}

Typical values: $Q_{1,p} = 200$, $Q_{1,v} = 1000$, $R_1 = 30$.

\textbf{Design rationale}:
\begin{itemize}
\item High $Q_{1,v}$ (velocity tracking): Ensures velocity matching with platoon
\item Moderate $Q_{1,p}$ (position tracking): Allows flexibility in positioning
\item Moderate $R_1$: Balances control smoothness with responsiveness
\end{itemize}

\subsection{Player 2 (Human Driver) Cost}

Player 2's cost is structurally identical but uses the human's reference trajectory:

\begin{equation}
J_2 = \sum_{j=0}^{N_p-1} \left\| \begin{bmatrix} p_{\text{ref},2}[k+j] \\ v_{\text{ref},2}[k+j] \end{bmatrix} - \begin{bmatrix} p[k+j] \\ v[k+j] \end{bmatrix} \right\|_{Q_2(\lambda)}^2 + \sum_{j=0}^{N_u-1} u_2[k+j]^2 R_2
\label{eq:cost_player2}
\end{equation}

\textbf{Critical coupling}: The output weight matrix for Player 2 depends on the authority ratio:

\begin{equation}
Q_2(\lambda[k]) = \lambda[k] \cdot Q_1
\label{eq:Q2_coupling}
\end{equation}

This dynamic weighting is the \textbf{key mechanism} that achieves cooperation:
\begin{itemize}
\item \textbf{Low risk} ($\lambda \to 0$): Player 2's tracking penalty is small $\to$ human has more freedom
\item \textbf{High risk} ($\lambda \to \infty$): Player 2's tracking penalty becomes large $\to$ human incentivized to follow system's reference
\end{itemize}

Typical control effort: $R_2 = 45$ (slightly higher than $R_1$ to prefer smoother human inputs).

\subsection{Compact Matrix Cost Representation}

Using Eq. \eqref{eq:compact_prediction}, define tracking errors:
\begin{align}
\mathbf{E}_1 &= \mathbf{R}_1 - \mathbf{Z} = \mathbf{R}_1 - (\mathbf{U} \mathbf{x}[k] + \mathbf{H}_1 D_1 + \mathbf{H}_2 D_2) \label{eq:error1}\\
\mathbf{E}_2 &= \mathbf{R}_2 - \mathbf{Z} = \mathbf{R}_2 - (\mathbf{U} \mathbf{x}[k] + \mathbf{H}_1 D_1 + \mathbf{H}_2 D_2) \label{eq:error2}
\end{align}

where $\mathbf{R}_1, \mathbf{R}_2 \in \mathbb{R}^{2N_p \times 1}$ are stacked reference trajectories.

Define block-diagonal weight matrices:
\begin{equation}
\bar{Q}_1 = \text{diag}(Q_1, Q_1, \ldots, Q_1) \in \mathbb{R}^{2N_p \times 2N_p}
\end{equation}
\begin{equation}
\bar{Q}_2 = \lambda[k] \cdot \bar{Q}_1
\end{equation}
\begin{equation}
\bar{R}_i = R_i \cdot \mathbf{I}_{N_u} \in \mathbb{R}^{N_u \times N_u}
\end{equation}

The costs become:
\begin{align}
J_1 &= \mathbf{E}_1^T \bar{Q}_1 \mathbf{E}_1 + D_1^T \bar{R}_1 D_1 \label{eq:cost_matrix_1}\\
J_2 &= \mathbf{E}_2^T \bar{Q}_2 \mathbf{E}_2 + D_2^T \bar{R}_2 D_2 \label{eq:cost_matrix_2}
\end{align}

\subsection{Adaptive Weight Tuning}

To improve performance across diverse scenarios, the base weights adapt dynamically:

\begin{equation}
Q_{1,p}[k] = Q_{1,p}^{\text{base}} \cdot \left(1 + \beta_{\text{gap}} \cdot \frac{|e_{\text{gap}}[k]|}{d_{\text{des}}}\right)
\label{eq:adaptive_qp}
\end{equation}

where $e_{\text{gap}}[k]$ is the gap error and $d_{\text{des}}$ is the desired spacing. This increases position tracking emphasis when the gap error is large.

Similarly, velocity tracking adapts to relative velocity:
\begin{equation}
Q_{1,v}[k] = Q_{1,v}^{\text{base}} \cdot \left(1 + \beta_v \cdot \frac{|v_{\text{rel}}[k]|}{v_{\text{ref}}}\right)
\label{eq:adaptive_qv}
\end{equation}

Typical adaptation gains: $\beta_{\text{gap}} = 0.5$, $\beta_v = 0.3$.

\section{Dynamic Authority Allocation}

\subsection{Lookup Table from Li et al. (2019)}

The authority ratio $\lambda[k]$ is computed from the safety field force $F_{\text{total}}[k]$ using an empirically-derived lookup table \cite{li2019shared}. The table maps force to $\ln(\lambda)$:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Force [N]} & \textbf{ln($\lambda$)} & $\lambda$ & \textbf{Human \%} & \textbf{System \%} \\
\hline
-400 & -4.27 & 0.014 & 98.6 & 1.4 \\
-200 & -3.56 & 0.028 & 97.3 & 2.7 \\
0 & -1.78 & 0.169 & 85.5 & 14.5 \\
100 & -0.83 & 0.436 & 69.6 & 30.4 \\
200 & 0.00 & 1.000 & 50.0 & 50.0 \\
300 & 0.63 & 1.878 & 34.7 & 65.3 \\
500 & 2.21 & 9.116 & 9.9 & 90.1 \\
800 & 4.23 & 68.717 & 1.4 & 98.6 \\
1000 & 4.67 & 106.6 & 0.9 & 99.1 \\
\hline
\end{tabular}
\caption{Authority allocation lookup table (selected values from Li et al. 2019)}
\label{tab:authority_table}
\end{table}

\textbf{Interpretation}:
\begin{itemize}
\item \textbf{Negative forces} (attractive): Vehicle too far, human dominant (formation keeping)
\item \textbf{Small positive forces} ($<$ 200 N): Slight risk, balanced authority
\item \textbf{Large positive forces} ($>$ 500 N): High risk, system dominant (safety intervention)
\end{itemize}

\subsection{Interpolation and Smoothing}

For arbitrary force values, linear interpolation is used:
\begin{equation}
\lambda[k] = \text{interp}(F_{\text{total}}[k], \mathbf{F}_{\text{table}}, \boldsymbol{\lambda}_{\text{table}})
\label{eq:lambda_interp}
\end{equation}

To prevent abrupt changes, low-pass filtering with rate limiting is applied:

\begin{equation}
\lambda_{\text{limited}}[k] = \text{clip}\left(\lambda_{\text{raw}}[k], \frac{\lambda[k-1]}{\gamma}, \gamma \cdot \lambda[k-1]\right)
\label{eq:rate_limit}
\end{equation}

\begin{equation}
\lambda[k] = \alpha_{\lambda} \lambda_{\text{limited}}[k] + (1 - \alpha_{\lambda}) \lambda[k-1]
\label{eq:lambda_smooth}
\end{equation}

with $\gamma = 1.5$ (max change factor) and $\alpha_{\lambda} \in [0.7, 0.9]$ (adaptive smoothing coefficient).

\textbf{Adaptive smoothing}: $\alpha_{\lambda}$ decreases when $\lambda$ is increasing rapidly (emergency) to allow faster response, and increases when decreasing (returning to normal) for stability.

\subsection{Conversion to Control Authority}

The shared control input uses the authority parameter $\alpha$:

\begin{equation}
\alpha[k] = \frac{\lambda[k]}{1 + \lambda[k]}
\label{eq:alpha_conversion}
\end{equation}

\begin{equation}
u_{\text{shared}}[k] = \alpha[k] \cdot u_1^*[k] + (1 - \alpha[k]) \cdot u_2^*[k]
\label{eq:shared_control}
\end{equation}

Properties of this blending:
\begin{itemize}
\item $\lambda \to 0$ $\Rightarrow$ $\alpha \to 0$ $\Rightarrow$ $u_{\text{shared}} \approx u_2$ (human control)
\item $\lambda \to \infty$ $\Rightarrow$ $\alpha \to 1$ $\Rightarrow$ $u_{\text{shared}} \approx u_1$ (system control)
\item $\lambda = 1$ $\Rightarrow$ $\alpha = 0.5$ $\Rightarrow$ equal weighting
\end{itemize}

\section{Nash Equilibrium Solution Algorithm}

\subsection{Best-Response Method}

The Nash equilibrium is computed iteratively using the \textbf{best-response algorithm}. Starting from initial guesses $D_1^{(0)}$ and $D_2^{(0)}$, each player alternately optimizes assuming the other's strategy is fixed:

\textbf{Iteration $\ell$}:
\begin{align}
D_1^{(\ell+1)} &= \arg\min_{D_1} J_1(D_1, D_2^{(\ell)}) \label{eq:best_response_1}\\
D_2^{(\ell+1)} &= \arg\min_{D_2} J_2(D_1^{(\ell+1)}, D_2) \label{eq:best_response_2}
\end{align}

This process continues until convergence.

\subsection{Analytical Solution via KKT Conditions}

Since the costs are quadratic and the system is linear, each best-response problem has a closed-form solution. Expanding Eq. \eqref{eq:cost_matrix_1}:

\begin{align}
J_1(D_1, D_2) &= (\mathbf{R}_1 - \mathbf{U} \mathbf{x} - \mathbf{H}_1 D_1 - \mathbf{H}_2 D_2)^T \bar{Q}_1 (\mathbf{R}_1 - \mathbf{U} \mathbf{x} - \mathbf{H}_1 D_1 - \mathbf{H}_2 D_2) \notag \\
&\quad + D_1^T \bar{R}_1 D_1
\end{align}

Define:
\begin{equation}
\mathbf{e}_1^{\text{free}} = \mathbf{R}_1 - \mathbf{U} \mathbf{x}
\end{equation}

Taking the gradient with respect to $D_1$ and setting to zero:

\begin{equation}
\frac{\partial J_1}{\partial D_1} = -2 \mathbf{H}_1^T \bar{Q}_1 (\mathbf{e}_1^{\text{free}} - \mathbf{H}_1 D_1 - \mathbf{H}_2 D_2) + 2 \bar{R}_1 D_1 = 0
\end{equation}

Rearranging:
\begin{equation}
(\mathbf{H}_1^T \bar{Q}_1 \mathbf{H}_1 + \bar{R}_1) D_1 = \mathbf{H}_1^T \bar{Q}_1 (\mathbf{e}_1^{\text{free}} - \mathbf{H}_2 D_2)
\label{eq:kkt_player1}
\end{equation}

Define Hessian and gradient:
\begin{align}
\mathbf{H}_{11} &= \mathbf{H}_1^T \bar{Q}_1 \mathbf{H}_1 + \bar{R}_1 \label{eq:H11}\\
\mathbf{H}_{12} &= \mathbf{H}_1^T \bar{Q}_1 \mathbf{H}_2 \label{eq:H12}\\
\mathbf{g}_1 &= \mathbf{H}_1^T \bar{Q}_1 \mathbf{e}_1^{\text{free}} \label{eq:g1}
\end{align}

Then:
\begin{equation}
D_1 = \mathbf{H}_{11}^{-1} (\mathbf{g}_1 - \mathbf{H}_{12} D_2)
\label{eq:d1_solution}
\end{equation}

Similarly for Player 2:
\begin{align}
\mathbf{H}_{22} &= \mathbf{H}_2^T \bar{Q}_2 \mathbf{H}_2 + \bar{R}_2 \label{eq:H22}\\
\mathbf{H}_{21} &= \mathbf{H}_2^T \bar{Q}_2 \mathbf{H}_1 \label{eq:H21}\\
\mathbf{g}_2 &= \mathbf{H}_2^T \bar{Q}_2 \mathbf{e}_2^{\text{free}} \label{eq:g2}
\end{align}

\begin{equation}
D_2 = \mathbf{H}_{22}^{-1} (\mathbf{g}_2 - \mathbf{H}_{21} D_1)
\label{eq:d2_solution}
\end{equation}

\subsection{Regularization for Numerical Stability}

To ensure $\mathbf{H}_{11}$ and $\mathbf{H}_{22}$ are well-conditioned, regularization is added:

\begin{equation}
\mathbf{H}_{ii}^{\text{reg}} = \mathbf{H}_{ii} + \epsilon_{\text{reg}} \mathbf{I}
\label{eq:regularization}
\end{equation}

where $\epsilon_{\text{reg}} \in [0.02, 0.05]$ adapts based on condition number:

\begin{equation}
\epsilon_{\text{reg}} = \begin{cases}
0.02 & \text{cond}(\mathbf{H}_{ii}) < 10^6 \\
0.05 & \text{cond}(\mathbf{H}_{ii}) \geq 10^6
\end{cases}
\label{eq:adaptive_reg}
\end{equation}

\subsection{Convergence Criteria}

The algorithm terminates when:

\begin{equation}
\max\left( \frac{\| D_1^{(\ell+1)} - D_1^{(\ell)} \|}{\| D_1^{(\ell)} \|}, \frac{\| D_2^{(\ell+1)} - D_2^{(\ell)} \|}{\| D_2^{(\ell)} \|} \right) < \epsilon_{\text{conv}}
\label{eq:convergence}
\end{equation}

with $\epsilon_{\text{conv}} = 0.1$ and maximum iterations $L_{\max} = 25$.

\textbf{Convergence analysis}: For the longitudinal double integrator with positive definite $Q_i$ and $R_i$, the cost functions are strictly convex in each player's decision variable. This guarantees:
\begin{enumerate}
\item Existence of a unique Nash equilibrium \cite{li2019shared}
\item Convergence of the best-response algorithm
\item Typical convergence in 5-10 iterations (observed empirically)
\end{enumerate}

\subsection{Complete Algorithm}

Algorithm \ref{alg:nash_solver} presents the complete procedure.

\begin{algorithm}[H]
\caption{Nash Equilibrium Solver via Best-Response}
\label{alg:nash_solver}
\begin{algorithmic}[1]
\State \textbf{Input:} Current state $\mathbf{x}[k]$, references $\mathbf{R}_1, \mathbf{R}_2$, authority ratio $\lambda[k]$
\State \textbf{Output:} Nash equilibrium controls $(u_1^*[k], u_2^*[k])$

\State // Initialization
\State $D_1^{(0)} \gets \mathbf{0}_{N_u}$, $D_2^{(0)} \gets \mathbf{0}_{N_u}$
\State Compute prediction matrices: $\mathbf{U}$, $\mathbf{H}_1$, $\mathbf{H}_2$ using Eq. \eqref{eq:U_matrix}-\eqref{eq:H1_matrix}
\State Compute free errors: $\mathbf{e}_1^{\text{free}} = \mathbf{R}_1 - \mathbf{U} \mathbf{x}[k]$, $\mathbf{e}_2^{\text{free}} = \mathbf{R}_2 - \mathbf{U} \mathbf{x}[k]$
\State Build weight matrices: $\bar{Q}_1$, $\bar{Q}_2 = \lambda[k] \bar{Q}_1$, $\bar{R}_1$, $\bar{R}_2$

\State // Compute Hessians and gradients
\State $\mathbf{H}_{11} \gets \mathbf{H}_1^T \bar{Q}_1 \mathbf{H}_1 + \bar{R}_1 + \epsilon_{\text{reg}} \mathbf{I}$
\State $\mathbf{H}_{12} \gets \mathbf{H}_1^T \bar{Q}_1 \mathbf{H}_2$
\State $\mathbf{g}_1 \gets \mathbf{H}_1^T \bar{Q}_1 \mathbf{e}_1^{\text{free}}$
\State $\mathbf{H}_{22} \gets \mathbf{H}_2^T \bar{Q}_2 \mathbf{H}_2 + \bar{R}_2 + \epsilon_{\text{reg}} \mathbf{I}$
\State $\mathbf{H}_{21} \gets \mathbf{H}_2^T \bar{Q}_2 \mathbf{H}_1$
\State $\mathbf{g}_2 \gets \mathbf{H}_2^T \bar{Q}_2 \mathbf{e}_2^{\text{free}}$

\State // Best-response iterations
\For{$\ell = 0$ to $L_{\max} - 1$}
    \State $D_1^{(\ell+1)} \gets \mathbf{H}_{11}^{-1} (\mathbf{g}_1 - \mathbf{H}_{12} D_2^{(\ell)})$ \Comment{Player 1 best response}
    \State $D_2^{(\ell+1)} \gets \mathbf{H}_{22}^{-1} (\mathbf{g}_2 - \mathbf{H}_{21} D_1^{(\ell+1)})$ \Comment{Player 2 best response}
    
    \State // Check convergence
    \State $\delta_1 \gets \| D_1^{(\ell+1)} - D_1^{(\ell)} \| / \| D_1^{(\ell)} \|$
    \State $\delta_2 \gets \| D_2^{(\ell+1)} - D_2^{(\ell)} \| / \| D_2^{(\ell)} \|$
    \If{$\max(\delta_1, \delta_2) < \epsilon_{\text{conv}}$}
        \State \textbf{break} \Comment{Converged}
    \EndIf
\EndFor

\State // Extract first controls
\State $u_1^*[k] \gets D_1^{(\ell+1)}[0]$
\State $u_2^*[k] \gets D_2^{(\ell+1)}[0]$

\State // Apply constraints
\State $u_1^*[k] \gets \text{clip}(u_1^*[k], u_{1,\min}, u_{1,\max})$
\State $u_2^*[k] \gets \text{clip}(u_2^*[k], u_{2,\min}, u_{2,\max})$

\State \Return $(u_1^*[k], u_2^*[k])$
\end{algorithmic}
\end{algorithm}

\section{Reference Trajectory Generation}

\subsection{System Reference (Player 1)}

The autonomous system's reference trajectory $\mathbf{R}_1$ is generated to maintain safe following distance using the Rajamani CACC model:

\begin{equation}
d_{\text{des}}[k] = d_0 + h \cdot v_{\text{ego}}[k]
\label{eq:rajamani_spacing}
\end{equation}

where $d_0 = 5$ m (standstill distance) and $h = 1.0$ s (time headway).

Target position and velocity:
\begin{align}
p_{\text{ref},1}[k+j] &= p_{\text{leader}}[k+j] - d_{\text{des}}[k+j] \label{eq:p_ref1}\\
v_{\text{ref},1}[k+j] &= v_{\text{leader}}[k+j] \label{eq:v_ref1}
\end{align}

The leader's future trajectory is predicted assuming constant velocity:
\begin{equation}
p_{\text{leader}}[k+j] = p_{\text{leader}}[k] + v_{\text{leader}}[k] \cdot j \cdot T_s
\end{equation}

For more accuracy, if leader acceleration is available:
\begin{equation}
p_{\text{leader}}[k+j] = p_{\text{leader}}[k] + v_{\text{leader}}[k] \cdot j T_s + \frac{1}{2} a_{\text{leader}}[k] \cdot (j T_s)^2
\end{equation}

\subsection{Human Reference (Player 2)}

The human driver's reference trajectory $\mathbf{R}_2$ is predicted by simulating the vehicle's behavior under platoon control over $N_p$ steps. This requires:

\begin{enumerate}
\item \textbf{Deep copy} of current simulation state (vehicle states, platoon structure)
\item \textbf{Multi-rate simulation}: Run physics at 50 Hz, sample outputs at 10 Hz
\item \textbf{Rajamani control}: Apply CACC control law to predict human's desired acceleration
\item \textbf{State propagation}: Integrate dynamics to obtain future position and velocity
\end{enumerate}

Prediction procedure:
\begin{equation}
\text{For } j = 0, 1, \ldots, N_p-1:
\end{equation}
\begin{align}
a_{\text{human}}[k+j] &= K_p (v_{\text{leader}} - v_{\text{human}}[k+j]) + K_d (\dot{d}_{\text{des}} - \dot{d}[k+j]) \notag \\
&\quad + K_i \int (d_{\text{des}} - d[k+j]) dt \label{eq:human_prediction}
\end{align}

Then:
\begin{align}
v_{\text{human}}[k+j+1] &= v_{\text{human}}[k+j] + a_{\text{human}}[k+j] \cdot T_s \\
p_{\text{human}}[k+j+1] &= p_{\text{human}}[k+j] + v_{\text{human}}[k+j] \cdot T_s + \frac{1}{2} a_{\text{human}}[k+j] \cdot T_s^2
\end{align}

The reference trajectory is:
\begin{equation}
\mathbf{R}_2 = \begin{bmatrix} p_{\text{human}}[k+1] \\ v_{\text{human}}[k+1] \\ \vdots \\ p_{\text{human}}[k+N_p] \\ v_{\text{human}}[k+N_p] \end{bmatrix}
\label{eq:R2_vector}
\end{equation}

\textbf{Computational cost}: Running $N_p = 20$ prediction steps with 5 substeps each requires 100 integration steps. This is feasible within the 100 ms control period (10 Hz rate).

\section{Implementation Considerations}

\subsection{Computational Complexity}

\textbf{Per control cycle (100 ms budget)}:
\begin{enumerate}
\item Safety field computation: $< 1$ ms (simple arithmetic)
\item Authority allocation lookup: $< 0.1$ ms (table interpolation)
\item Reference generation: 20-30 ms (simulation-based prediction)
\item Prediction matrix construction: 5 ms (one-time per horizon)
\item Nash iterations (5-10 typical): 20-40 ms (linear solves)
\item Total: 50-80 ms $\to$ within budget
\end{enumerate}

\subsection{Numerical Stability Checklist}

\begin{itemize}
\item Regularization: $\epsilon_{\text{reg}} \geq 0.02$
\item Condition number monitoring: Warn if $\text{cond}(\mathbf{H}_{ii}) > 10^8$
\item Control clipping: $|u_i| \leq u_{\max}$
\item Lambda bounds: $0.01 \leq \lambda \leq 150$
\item Convergence detection: Use relative tolerance, not absolute
\end{itemize}

\subsection{Edge Cases}

\begin{enumerate}
\item \textbf{Nash solver divergence}: If convergence not achieved in $L_{\max}$ iterations, use last iterate with warning
\item \textbf{Singular Hessian}: Increase $\epsilon_{\text{reg}}$ dynamically
\item \textbf{Large velocity errors}: Temporarily increase $Q_{1,v}$ for faster convergence
\item \textbf{Emergency situations}: Override Nash output with hard constraints (Chapter 3)
\end{enumerate}

\section{Summary}

This chapter presented the complete game-theoretic framework:

\begin{itemize}
\item \textbf{Nash game formulation}: Non-cooperative two-player game with coupled cost functions
\item \textbf{DMPC prediction}: Compact matrix form with multi-rate architecture
\item \textbf{Cost functions}: Quadratic costs with dynamic weighting via $\lambda[k]$
\item \textbf{Authority allocation}: Lookup table mapping safety field force to authority ratio
\item \textbf{Best-response algorithm}: Iterative analytical solution with guaranteed convergence
\item \textbf{Reference generation}: System reference from CACC, human reference from simulation
\end{itemize}

Key innovations: (1) dynamic cost weighting achieves cooperation in non-cooperative framework, (2) authority allocation smoothly transitions control based on risk, (3) best-response converges rapidly (5-10 iterations), (4) multi-rate architecture balances accuracy and efficiency.

Chapter 5 describes the complete software implementation, including integration with the Unity simulator and Python control modules.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Prediction horizon $N_p$ & 20 steps (2 s) \\
Control horizon $N_u$ & 12 steps (1.2 s) \\
Control time step $T_s$ & 0.1 s \\
Base $Q_{1,p}$ & 200 \\
Base $Q_{1,v}$ & 1000 \\
Control cost $R_1$ & 30 \\
Control cost $R_2$ & 45 \\
Regularization $\epsilon_{\text{reg}}$ & 0.02-0.05 \\
Convergence tolerance $\epsilon_{\text{conv}}$ & 0.1 \\
Max iterations $L_{\max}$ & 25 \\
Smoothing $\alpha_{\lambda}$ & 0.7-0.9 \\
Rate limit factor $\gamma$ & 1.5 \\
\hline
\end{tabular}
\caption{Key parameters for Nash equilibrium solver}
\label{tab:nash_parameters}
\end{table}


%--------------------------------------------------------------
% BIBLIOGRAPHY 
%--------------------------------------------------------------

\printbibliography

%--------------------------------------------------------------
% APPENDIX
%--------------------------------------------------------------
\appendix

\chapter{Python Implementation}

The complete Python implementation is available at: \texttt{[Repository link]}

\textbf{Key modules:}
\begin{itemize}
\item \texttt{longitudinal\_nash\_solver.py}: Nash equilibrium solver with DMPC
\item \texttt{longitudinal\_safety\_field.py}: Bidirectional safety field computation
\item \texttt{longitudinal\_authority\_allocator.py}: Dynamic authority allocation
\item \texttt{main\_with\_nash.py}: Complete system integration
\end{itemize}

\end{document}










%--------------------------------------------------------------
% BIBLIOGRAPHY
%--------------------------------------------------------------
\begin{thebibliography}{99}

\bibitem{abdElmoniem2020stanley}
AbdElmoniem, A., Osama, A., Abdelaziz, M., \& Maged, S. A. (2020).
\textit{A path-tracking algorithm using predictive Stanley lateral controller}.
International Journal of Advanced Robotic Systems, 17(6), 1-12.

\bibitem{fernandez2012vehicle}
Fernández, D. S. (2012).
\textit{A Vehicle Dynamics Model for Driving Simulators}.
Master's Thesis, Chalmers University of Technology.

\bibitem{gu2014humanlike}
Gu, T., \& Dolan, J. M. (2014).
\textit{Toward Human-like Motion Planning in Urban Environments}.
In IEEE Intelligent Vehicles Symposium (IV), 350-355.

\bibitem{hoffmann2007stanley}
Hoffmann, G. M., Tomlin, C. J., Montemerlo, M., \& Thrun, S. (2007).
\textit{Autonomous Automobile Trajectory Tracking for Off-Road Driving: Controller Design, Experimental Validation and Racing}.
In American Control Conference, 2296-2301.

\bibitem{kesting2007mobil}
Kesting, A., Treiber, M., \& Helbing, D. (2007).
\textit{General Lane-Changing Model MOBIL for Car-Following Models}.
Transportation Research Record: Journal of the Transportation Research Board, 1999(1), 86-94.

\bibitem{li2019shared}
Li, M., Cao, H., Song, X., Huang, Y., Wang, J., \& Huang, Z. (2019).
\textit{Shared control with a novel dynamic authority allocation strategy based on game theory and driving safety field}.
Mechanical Systems and Signal Processing, 124, 199-216.

\bibitem{song2013vehicle}
Song, X., Cao, H., \& Huang, J. (2013).
\textit{Vehicle path planning in various driving situations based on the elastic band theory for highway collision avoidance}.
Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering, 227(12), 1706-1722.

\bibitem{thrun2006stanley}
Thrun, S., Montemerlo, M., Dahlkamp, H., Stavens, D., Aron, A., Diebel, J., Fong, P., Gale, J., Halpenny, M., Hoffmann, G., Lau, K., Oakley, C., Palatucci, M., Pratt, V., Stang, P., Strohband, S., Dupont, C., Jendrossek, L.-E., Koelen, C., Markey, C., Rummel, C., van Niekerk, J., Jensen, E., Alessandrini, P., Bradski, G., Davies, B., Ettinger, S., Kaehler, A., Nefian, A., \& Mahoney, P. (2006).
\textit{Stanley: The Robot that Won the DARPA Grand Challenge}.
Journal of Field Robotics, 23(9), 661-692.

\bibitem{treiber2013traffic}
Treiber, M., \& Kesting, A. (2013).
\textit{Traffic Flow Dynamics: Data, Models and Simulation}.
Springer-Verlag Berlin Heidelberg.

\bibitem{treiber2000idm}
Treiber, M., Hennecke, A., \& Helbing, D. (2000).
\textit{Congested Traffic States in Empirical Observations and Microscopic Simulations}.
Physical Review E, 62(2), 1805-1824.

\bibitem{rajamani2012vehicle}
Rajamani, R. (2012).
\textit{Vehicle Dynamics and Control} (2nd ed.).
Springer Science+Business Media.

\bibitem{wang2015driving}
Wang, J., Wu, J., \& Li, Y. (2015).
\textit{The Driving Safety Field Based on Driver–Vehicle–Road Interactions}.
IEEE Transactions on Intelligent Transportation Systems, 16(4), 2203-2214.

\bibitem{wang2016driving}
Wang, J., Wu, J., Zheng, X., Ni, D., \& Li, K. (2016).
\textit{Driving safety field theory modeling and its application in pre-collision warning system}.
Transportation Research Part C: Emerging Technologies, 72, 306-324.

\bibitem{kesting2007mobil2}
Kesting, A., Treiber, M., \& Helbing, D. (2007).
\textit{MOBIL: General Lane-Changing Model for Car-Following Models}.
TRB 86th Annual Meeting Compendium of Papers.

\end{thebibliography}

%--------------------------------------------------------------
% APPENDIX
%--------------------------------------------------------------
\appendix

\chapter{Python Implementation}

The complete Python implementation is available at: \texttt{[Repository link]}

\textbf{Key modules:}
\begin{itemize}
\item \texttt{enhanced\_nash\_solver.py}: Nash equilibrium solver
\item \texttt{ellipse\_based\_longitudinal\_safety\_field.py}: Safety field computation
\item \texttt{lateral\_controller.py}: Lateral MPC with bicycle model
\item \texttt{main\_simulation.py}: System integration
\end{itemize}

\end{document}








